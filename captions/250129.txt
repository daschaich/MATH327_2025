1
00:00:01,500 --> 00:00:05,870
[Auto-generated transcript. Edits may have been applied for clarity.]
Okay, I've got the recording going, so

2
00:00:05,880 --> 00:00:10,680
I think we can get underway. Welcome back. I'm glad you're back to stat mech.

3
00:00:11,220 --> 00:00:19,379
We have two hours today where we will start to get into more of the swing of things, rather than just the intro and logistics.

4
00:00:19,380 --> 00:00:23,790
And it looks like this. Mike is out of battery already. That didn't last long.

5
00:00:24,840 --> 00:00:28,470
Hopefully this will come up on the recording for those of you who need it.

6
00:00:29,160 --> 00:00:36,540
Um, to our lecture today. That means that we'll do a ten minute break after roughly 50 minutes.

7
00:00:37,080 --> 00:00:42,360
I have some flexibility to decide exactly what to do that, but we'll give you a chance to wake up,

8
00:00:42,360 --> 00:00:48,030
stretch your legs, and ask any questions you have at that time if you don't feel brave enough to shout them out.

9
00:00:48,840 --> 00:00:53,970
Uh, while the recording is going on, it's the first thing today I need to remember.

10
00:00:55,540 --> 00:01:04,540
To get the magic number on here in a way that all of you can see and check in.

11
00:01:04,540 --> 00:01:08,920
If there are any immediate questions about what we talked about on Monday.

12
00:01:09,490 --> 00:01:15,310
And that was most of the logistics and questions about logistics are certainly welcome.

13
00:01:15,550 --> 00:01:22,630
You know, in addition to that, we talked about some of the big ideas of this module, the idea of.

14
00:01:24,600 --> 00:01:35,190
Large scale macroscopic behaviour emerging from the microscopic dynamical dynamics of many degrees of freedom or particles.

15
00:01:36,060 --> 00:01:46,980
Some examples of that in ways it is very relevant to research and investigations being done in modern days.

16
00:01:47,820 --> 00:01:53,490
It's warm in here, so I'll see if I can crack some of these windows.

17
00:01:54,090 --> 00:02:02,010
I'll also say when the sun gets into the west and shines through these windows, sometimes it makes the.com completely illegible.

18
00:02:02,430 --> 00:02:06,020
So let me know if that happens. Hopefully we can keep the windows open.

19
00:02:06,030 --> 00:02:11,130
Have some natural light for you. If it causes trouble, we'll take action to fix that.

20
00:02:13,110 --> 00:02:19,530
We just started with some of the more formal probability foundations that are going

21
00:02:19,530 --> 00:02:25,440
to be a bit different than the kind of typical content we'll be covering in this,

22
00:02:25,920 --> 00:02:30,000
this module, but it is a starting point and a, well, a bit of formal mathematics,

23
00:02:30,240 --> 00:02:34,590
some statements of definitions and some theorems that we won't really bother to prove.

24
00:02:35,190 --> 00:02:43,470
Um, with a more of a physicist perspective. But we started off by defining the idea of a random experiment.

25
00:02:45,800 --> 00:02:54,980
This. Script E which when we carry it out, it is some manipulation or observation of the universe.

26
00:02:55,400 --> 00:02:58,580
It leaves the universe in some state. Omega.

27
00:02:59,540 --> 00:03:08,300
And this Omega is an element in a capital Omega set of all states, which is.

28
00:03:09,970 --> 00:03:15,130
Collecting all of the possible outcomes of the experiment or observation that we are doing.

29
00:03:16,600 --> 00:03:25,930
So we have many more definitions to get through today, and that should not take all of or even most of the time.

30
00:03:26,650 --> 00:03:33,730
But we will formalise the definition of what are called probability spaces built on top of these random experiments,

31
00:03:34,150 --> 00:03:42,639
and then see briefly a few useful tools known as the law of large numbers and central limit theorem,

32
00:03:42,640 --> 00:03:54,160
which are quite possibly things you have seen before in other modules or potentially even A-level further maths, depending on on your background.

33
00:03:54,550 --> 00:04:02,200
We're not going to be interested really in proving these in full detail, but seeing the qualitative concepts,

34
00:04:02,200 --> 00:04:09,730
how they work and then how they can be applied to other sorts of statistical mechanics,

35
00:04:10,180 --> 00:04:14,860
uh, probabilistic systems that we will be interested in for this module.

36
00:04:14,860 --> 00:04:17,890
The first example of that are going to be.

37
00:04:20,040 --> 00:04:31,260
Things known as random walks. And if we don't get to random walks by the end of today, then they will be the topic for next week,

38
00:04:31,410 --> 00:04:39,840
and in particular the computer labs and the computer assignments that, um, you'll have those labs to work on starting next Thursday.

39
00:04:39,840 --> 00:04:46,530
A week from tomorrow. We'll deal in maybe more detail than you want with these random walks.

40
00:04:48,540 --> 00:05:00,060
So any questions about what we did on Monday, what the plan is for today before we charge ahead with the next definition in our sequence.

41
00:05:04,670 --> 00:05:06,650
That definition is going to be.

42
00:05:09,380 --> 00:05:20,540
Measurement as a formal mathematical concept, where when we look at the states of the universe that we get after a random experiment,

43
00:05:20,840 --> 00:05:28,730
you know, I talked about flipping a coin on Monday, and the states can include information not only about the orientation with which the coin lands,

44
00:05:29,120 --> 00:05:33,620
but also the position, the temperature at the time that that coin flip is done.

45
00:05:34,370 --> 00:05:42,920
If we want to extract the information of interest, we carry out a measurement on the state that our experiment produces.

46
00:05:43,730 --> 00:05:50,299
So this is a function that acts on the state of the universe and just gives us or

47
00:05:50,300 --> 00:05:56,990
extracts the information that we are actually interested in for mathematical analysis.

48
00:05:58,760 --> 00:06:08,510
So for that coin flip very well, you know, almost by instinct, just care about what side of the coin is facing up heads or tails,

49
00:06:09,260 --> 00:06:17,120
um, and discard all of the additional information that is not going into the subsequent analysis that we want to do.

50
00:06:18,890 --> 00:06:27,140
If we imagine repeating a random experiment many times, then we get some sequence of states out.

51
00:06:27,590 --> 00:06:36,319
We measure each of them and the measurement on all of those states, which I have gone ahead and labelled with this index.

52
00:06:36,320 --> 00:06:47,840
I, um, just something that we can do as a way to distinguish even between, uh, countably many, a countably infinite number of states.

53
00:06:48,740 --> 00:06:55,340
This measurement outcome of the states, due to the randomness inherent in the experiment will randomly fluctuate.

54
00:06:56,000 --> 00:07:07,870
So this is. A random variable that we can then apply these tools and techniques of probability theory to analyse.

55
00:07:09,520 --> 00:07:17,380
And if we imagine carrying out our measurement on all possible states that our experiment can produce,

56
00:07:17,920 --> 00:07:22,030
then we will obtain all possible outcomes that we measure.

57
00:07:22,510 --> 00:07:26,020
And we will call this a different set A.

58
00:07:26,380 --> 00:07:32,880
So this is now. The set of all measurements on all possible states.

59
00:07:34,780 --> 00:07:38,440
Either the set of all outcomes or the outcome space.

60
00:07:40,800 --> 00:07:46,320
As another definition. And just like the possible states that we can get.

61
00:07:46,590 --> 00:07:53,070
We can imagine outcome spaces that have a finite number of measurements outcomes.

62
00:07:53,220 --> 00:08:00,810
Flipping a coin is like that. Heads or tails to two elements in this space, nice and finite, to deal with.

63
00:08:01,230 --> 00:08:09,300
We can also have a countably infinite number of states or a number of outcomes coming from these states.

64
00:08:10,710 --> 00:08:20,160
We can even imagine, um, or not just imagine, but deal with real valued outcomes where,

65
00:08:20,160 --> 00:08:26,070
say, we measure a position with, in principle, arbitrary precision along the real axis.

66
00:08:26,580 --> 00:08:34,680
So these outcome spaces, like the set of all states, can be finite, infinite, countable, or continuous.

67
00:08:35,100 --> 00:08:47,070
All of those are possibilities that these definitions are going to be handling, and that we will see and deal with in the course of our work.

68
00:08:47,100 --> 00:08:59,640
Let's just. Do a few simple examples to make sure that everything is clear, and give some intuition to having these kind of formal definitions off of.

69
00:09:00,510 --> 00:09:05,880
So maybe going one step above flipping a coin.

70
00:09:06,330 --> 00:09:16,950
If we imagine that our experiment is rolling a six sided die and the measurement that we do on that,

71
00:09:17,160 --> 00:09:24,180
or after carrying out that experiment is to just look at the number that comes out on top of the die.

72
00:09:27,550 --> 00:09:37,690
Then we can. Figure out the outcome space just by figuring out all the numbers that can come out on top.

73
00:09:38,080 --> 00:09:48,610
And maybe, as a brief aside, one aspect of measurement that um, I didn't mention here but should,

74
00:09:48,970 --> 00:09:56,430
um, should mention, at least in passing formally, is that we have to worry about the possibility.

75
00:09:56,440 --> 00:10:01,750
What happens if the information of interest is really not present in the state that we get out?

76
00:10:02,050 --> 00:10:09,010
So, for example, if we flip a coin and want to measure heads or tails, but it lands on its side, what do we do?

77
00:10:09,010 --> 00:10:13,600
In that case? The measurement still is general enough to apply.

78
00:10:13,630 --> 00:10:19,510
It kind of acts as a filter and says that if the information of interest is not present.

79
00:10:20,050 --> 00:10:27,940
So for instance, if our die that we roll falls off the table and down the down a super great and we can't tell which number is on top,

80
00:10:28,450 --> 00:10:31,630
or if we roll a die and a dog comes along and eats it and we can't tell,

81
00:10:31,930 --> 00:10:39,760
then we just discard that experiment and can proceed with measurements as random variables within the outcome space.

82
00:10:40,400 --> 00:10:43,660
Um, you might already have written down the outcome space for rolling a died.

83
00:10:44,260 --> 00:10:47,890
Does anyone want to shout it out if we measure the number on top?

84
00:10:50,390 --> 00:10:55,430
12345. It is exactly one, two, three, four, five and six.

85
00:10:59,120 --> 00:11:04,520
So and that is extracted from broader states.

86
00:11:05,000 --> 00:11:11,930
That's like with flipping a coin could also include information about time temperature.

87
00:11:14,020 --> 00:11:17,710
Um, position of the di. So this is.

88
00:11:25,750 --> 00:11:32,710
Just a very simple, um, thing to keep in mind when or.

89
00:11:34,460 --> 00:11:44,980
And trying to pull up these definitions. I find it useful at least to have some simple examples to kind of hang those definitions off of.

90
00:11:45,000 --> 00:11:48,560
Makes it easier for me to remember than the more formal statement.

91
00:11:51,300 --> 00:12:00,900
That distinction between outcome and state. And well, let's do a couple more that can add a few more wrinkles.

92
00:12:01,320 --> 00:12:13,780
So one. Slightly more involved experiment we could do only slightly is to not just flip a coin and look at heads or tails,

93
00:12:13,780 --> 00:12:21,760
but to flip that coin four times, or flip four different coins and measure the.

94
00:12:24,350 --> 00:12:31,190
Uh, heads or tails orientation of every single one of those flips, whether they're all done at once or one after the next.

95
00:12:31,670 --> 00:12:39,200
So our measurement is to look at, uh, for heads versus tails.

96
00:12:42,370 --> 00:12:48,580
And you can shout out the outcome space again.

97
00:12:49,480 --> 00:12:53,050
Maybe just a single element in it. What would it look like?

98
00:12:57,360 --> 00:13:05,160
Maybe that's. That is exactly the sort of outcome that we would have in our outcome space.

99
00:13:06,030 --> 00:13:12,450
And well, all possibilities are in this set of all outcomes that come from the set of all states.

100
00:13:13,080 --> 00:13:17,190
So all four heads. All four tails.

101
00:13:19,510 --> 00:13:24,590
And as well, the order can be something that that matters.

102
00:13:24,610 --> 00:13:28,300
We can distinguish between the first flip, the second flip, and so on.

103
00:13:28,780 --> 00:13:35,650
So heads tails heads tails can be a distinct element from heads heads tails tails.

104
00:13:36,340 --> 00:13:39,910
There are more of these I won't write them out. How many are there?

105
00:13:43,310 --> 00:14:17,340
Thought I'd heard. It's already. No need to be shy.

106
00:14:17,360 --> 00:14:29,660
If you have an idea for the number of elements that we have in this outcome set for four coin flips, measuring heads or tails each time.

107
00:15:03,410 --> 00:15:07,460
It is possible to write down all elements and count them. There is an easier way.

108
00:15:07,940 --> 00:15:15,870
If any of you are doing that. Give you that hint. It's just 16.

109
00:15:16,660 --> 00:15:26,230
It is. And the way that comes out is to note that each of the coin flips has two possible outcomes heads or tails.

110
00:15:26,530 --> 00:15:27,880
And we do that four times.

111
00:15:28,330 --> 00:15:37,630
So rather, you know, the only complication is that we have to multiply all of those factors of two rather than say, adding them.

112
00:15:38,020 --> 00:15:42,280
We get two for the first flip multiplied by two for the second.

113
00:15:42,550 --> 00:15:46,600
In each of those four cases, two for the third and then two for the fourth.

114
00:15:48,360 --> 00:15:52,780
Um, and that sort of counting is useful too.

115
00:15:53,590 --> 00:15:58,120
Well, it's something we'll see frequently and in more general situations.

116
00:15:58,630 --> 00:16:03,709
So. Good to be confident in that.

117
00:16:03,710 --> 00:16:14,360
And also note that all of these 16 elements are distinct, because we have specified that the order of heads versus tails does matter.

118
00:16:15,020 --> 00:16:23,210
Um, we'll check examples with random walks, for example, where the order doesn't matter, we only care about the total number.

119
00:16:23,720 --> 00:16:30,140
Um, and that will be a simple generalisation coming along earlier,

120
00:16:30,140 --> 00:16:42,140
the sort of examples that we might also want to keep in mind for statistical physics in thermodynamics is, say, ten to the power 23 atoms.

121
00:16:42,170 --> 00:16:50,990
Let's say they're all argon atoms, and they are just bouncing around in some container.

122
00:16:51,440 --> 00:16:55,970
So we have our airtight box.

123
00:16:56,540 --> 00:17:03,349
We have a bunch of atoms that are in there all bouncing around.

124
00:17:03,350 --> 00:17:06,500
I will draw on ten to the power, 23 of them.

125
00:17:07,940 --> 00:17:19,550
But, uh, you know, the state that we could get out of the experiment of just looking at all of these atoms in this container.

126
00:17:22,380 --> 00:17:30,750
Could include all sorts of information about the microscopic dynamics of these ten to the power 23 atoms.

127
00:17:30,750 --> 00:17:35,250
So they all have some position in three dimensional space.

128
00:17:36,120 --> 00:17:41,639
And they drew two. But you can imagine just a cubic box.

129
00:17:41,640 --> 00:17:47,040
They all have their. Own velocities and corresponding momenta.

130
00:17:47,850 --> 00:17:53,730
Um, different atoms could be in different electronic states or could have different isotopes.

131
00:17:58,130 --> 00:18:04,280
So all of that is kind of the full information about the state if we imagine being able to.

132
00:18:06,450 --> 00:18:11,669
To measure it in full detail. Uh, what sorts of things I will ask you.

133
00:18:11,670 --> 00:18:17,610
You would be interested. It would be interesting to measure in this sort of setup.

134
00:18:22,910 --> 00:18:28,470
Think the pressure. Pressure is a great one, which we will see.

135
00:18:30,530 --> 00:18:33,950
In detail later in this module. Anymore.

136
00:18:42,150 --> 00:18:50,190
The temperature. Temperature is also one of these so-called thermodynamic quantities that.

137
00:18:51,960 --> 00:19:01,200
Encapsulates in a small number of concepts the large scale behaviour of all of this information that we have.

138
00:19:01,920 --> 00:19:11,459
Another one that I can add on related to temperature and pressure is the so-called internal energy of all of these atoms.

139
00:19:11,460 --> 00:19:17,000
So that's I say it's internal and that is terminology that will be important.

140
00:19:17,010 --> 00:19:21,930
It has to do with just what's going on inside the box and is not affected by,

141
00:19:21,930 --> 00:19:27,030
say, if we put that box on the train and accelerate it to 100 miles an hour,

142
00:19:27,450 --> 00:19:33,899
that changes the energy from an external perspective, but not the internal energy of the box in that rest frame.

143
00:19:33,900 --> 00:19:38,490
Similarly, if we put it into the orbit of a black hole, its potential energy changes.

144
00:19:38,760 --> 00:19:50,280
Internal energy does not. There are more things we will see, like the heat capacity that we talked about on Monday as a target.

145
00:19:50,610 --> 00:19:55,470
This is the amount that the energy changes given a change in the temperature.

146
00:19:56,280 --> 00:20:01,799
We could even generalise and look at things like large scale movements of these atoms.

147
00:20:01,800 --> 00:20:06,330
If there are currents or convection in this box,

148
00:20:07,080 --> 00:20:17,040
that is something that is more complicated to study because it's not necessarily stable and static, but actually dynamic and changing.

149
00:20:17,490 --> 00:20:23,549
But it is something that is interesting to measure in getting more toward actual research.

150
00:20:23,550 --> 00:20:36,570
If you want to predict, say, how to make a plastic widget in a mould, you have to figure out how when you inject the molten plastic into the mould,

151
00:20:36,900 --> 00:20:42,630
it will flow and settle down in there so that it has a uniform consistency.

152
00:20:43,260 --> 00:20:47,010
Um. So there's lots that can be done,

153
00:20:47,460 --> 00:20:58,680
including the thermodynamic stuff we that we will do as well as then more and more information in the application up to really cutting edge stuff.

154
00:20:59,520 --> 00:21:05,820
So those are simple examples to hang things off of. Let's blast through some more definitions.

155
00:21:06,090 --> 00:21:17,670
Get them out of the way. The little prompt I had to consider as you came in was talking about estimating the probabilities of events.

156
00:21:18,000 --> 00:21:24,000
And if we want to be very formal mathematicians, we have to define all of those concepts,

157
00:21:24,000 --> 00:21:33,390
including what does an event mean in the context of outcome spaces and event space or probability spaces?

158
00:21:34,080 --> 00:21:45,640
Um. So this event we will define as any subset of the of the outcome space A.

159
00:21:52,680 --> 00:22:05,940
So just with that example of rolling a die where we had a being one through six and the number of measured on top possible events in that example.

160
00:22:10,070 --> 00:22:19,520
So from rolling a die. Could be rolling.

161
00:22:19,520 --> 00:22:24,080
The rolling a six. That is an element in the outcome space.

162
00:22:25,010 --> 00:22:32,720
A more general event is rolling. Anything buddy six so any number one through five inclusive.

163
00:22:33,170 --> 00:22:39,320
Or we could imagine rolling an even number or an odd number.

164
00:22:39,440 --> 00:22:49,550
All of those are different possible events that we can define, each of which comes with its own probability that we are building up to.

165
00:22:50,330 --> 00:23:02,090
So the event is now a generalisation of the outcomes that can be one or more outcomes coming from that set of all outcomes in the outcome space.

166
00:23:02,090 --> 00:23:12,230
And there is similarly. Just as there is a set of all states and an outcome space, there is an event space, which is.

167
00:23:13,970 --> 00:23:19,340
A script. F is the notation I will use for that, and this is the set.

168
00:23:22,170 --> 00:23:26,830
Of all events. That we actually care about.

169
00:23:26,840 --> 00:23:31,420
So not necessarily all possible events that we can.

170
00:23:32,800 --> 00:23:38,110
Construct by considering all possible combinations of elements in the outcome space,

171
00:23:38,740 --> 00:23:51,190
but rather a subset of those full possibilities that we specify as the events that we are actually interested in.

172
00:23:54,850 --> 00:24:08,260
So we will be imposing our will, becoming more active and actually specifying what events we want to look at when we consider their probabilities.

173
00:24:09,070 --> 00:24:15,070
So getting toward our final definitions for today,

174
00:24:16,060 --> 00:24:27,310
the probability is a measure function that assigns a number to each of the events within the event space.

175
00:24:29,440 --> 00:24:41,800
So calling it P it takes us from that set f two numbers between 0% and 100% zero and one inclusive.

176
00:24:43,570 --> 00:24:55,000
It associates just a real number in that range for each of the events that we have specified are of interest to us.

177
00:24:55,360 --> 00:24:58,780
So in the event space F, this is.

178
00:25:00,640 --> 00:25:04,520
Not quite sufficient for a definition of probability.

179
00:25:04,540 --> 00:25:16,599
There are two requirements that we have to impose in order to make this measure function well-defined, and match up to your everyday intuition.

180
00:25:16,600 --> 00:25:20,200
When we talk about probabilities, just in, um.

181
00:25:22,410 --> 00:25:35,760
In colloquial enough formal mathematical terms. So first is that if we look at the probability of a combination of events in our event space.

182
00:25:35,850 --> 00:25:50,040
So if we want to know the probability of having events x or y or Z where we will not just have a countable list in the this sort of selection,

183
00:25:50,040 --> 00:25:55,140
just so that we can iterate it with ORS. And we also.

184
00:25:57,600 --> 00:26:06,700
Need to require that. The possible events we consider here are mutually exclusive and not overlapping.

185
00:26:08,290 --> 00:26:12,250
And the pop ups and the screen saver. So there is.

186
00:26:16,440 --> 00:26:21,460
I think that came up yesterday as well. Um, thanks for flagging that.

187
00:26:21,840 --> 00:26:27,100
So always do let me know if you can see the other side of the equal sign.

188
00:26:27,100 --> 00:26:31,810
Here is what you would expect. Given these conditions of having countable and mutually exclusive events,

189
00:26:32,470 --> 00:26:43,780
we require that the probability of a combination of events is just the probability of the individual events that we have in there.

190
00:26:43,780 --> 00:26:52,810
So if we in this example talk about the probability of rolling any number one through five and six, then we can put that up,

191
00:26:53,140 --> 00:26:59,550
pull that apart into the probability of rolling a six plus the probability of rolling one through five.

192
00:26:59,560 --> 00:27:02,560
Those are mutually exclusive probabilities.

193
00:27:03,130 --> 00:27:11,700
And then. Also not too surprising if we look at the probability of the entire outcome.

194
00:27:11,700 --> 00:27:25,229
Space. Hey. Hmm. Um, if we set our events, our event space F to be each individual outcome, no combinations of them.

195
00:27:25,230 --> 00:27:32,700
So F being equal to A would be one, two, three, four, five, six as six separate elements in the rolling a die example.

196
00:27:33,360 --> 00:27:45,210
Then we have the requirement that the probability of all individual distinct outcomes by construction have to add up to over one 100%,

197
00:27:45,660 --> 00:28:00,150
which is just saying that. The experiments and measurement that we do must have some measurable outcome in order to have happened in the first place.

198
00:28:12,450 --> 00:28:18,570
So putting all of these definitions together.

199
00:28:24,110 --> 00:28:27,380
We get something called a probability space, which.

200
00:28:29,720 --> 00:28:32,930
Is going to be just the.

201
00:28:34,740 --> 00:28:38,910
Collection of these objects that we have been defining.

202
00:28:39,540 --> 00:28:43,170
We have our set of outcomes. Um.

203
00:28:43,560 --> 00:28:48,090
We could have also included with this the set of all states. But as the outcomes that we will be interested in.

204
00:28:48,090 --> 00:28:54,510
So I am going to start being less formal and using those sort of interchangeably from our outcomes.

205
00:28:55,260 --> 00:29:01,800
We can choose events of interest. Each of those events comes with a probability attached.

206
00:29:02,400 --> 00:29:06,660
And all of that together gives us our probability space.

207
00:29:07,240 --> 00:29:14,700
So that specifies the probability for all events that we are interested in.

208
00:29:15,030 --> 00:29:25,980
All of the interesting subsets of the possible outcomes of our experiment followed by that measurement.

209
00:29:30,280 --> 00:29:36,549
So any questions about just these formal definitions for we will do some more

210
00:29:36,550 --> 00:29:42,580
examples and then talk about ways of determining or estimating these probabilities.

211
00:29:49,940 --> 00:29:58,070
And so far, even though this is fairly formal, it should, I think, all align pretty well to an everyday intuition.

212
00:29:58,610 --> 00:30:02,740
So for simplicity you will.

213
00:30:02,750 --> 00:30:12,799
For the time being, suppose that we have a finite set of possible outcomes of our sphere of our experiment.

214
00:30:12,800 --> 00:30:16,580
I should also say finite set of possible states. I'm already.

215
00:30:19,080 --> 00:30:23,790
Using these interchangeably. If we have an outcome.

216
00:30:24,390 --> 00:30:30,660
Sorry. If we have a set of all states that has some finite number, capital and elements,

217
00:30:31,350 --> 00:30:40,680
then this implies and requires that the outcome space itself must also be finite.

218
00:30:42,430 --> 00:30:48,820
Um, well, no matter what we measure here, there are only going to be a finite number of possibilities.

219
00:30:49,270 --> 00:30:53,710
And in general, if we say that.

220
00:30:55,800 --> 00:31:02,250
As a brief label are X acting on each of these states is some capital X outcome?

221
00:31:03,600 --> 00:31:10,740
I will just call the outcome space x1, x2 and up to a lowercase n.

222
00:31:12,030 --> 00:31:16,230
So this lowercase n is at most the capital n.

223
00:31:16,500 --> 00:31:23,490
The total number of outcomes we can have is no more than the total number of states we have to measure in general.

224
00:31:25,200 --> 00:31:38,840
Usually we will have um. And strictly less little and strictly less than capital N, and usually very much smaller.

225
00:31:40,730 --> 00:31:42,050
And this is just because.

226
00:31:45,180 --> 00:31:55,020
When we apply our measurements to extract the information of interest and discard all of the information that we are not interested in,

227
00:31:55,440 --> 00:31:59,910
we will typically find many different states that give the same outcome measurements.

228
00:31:59,920 --> 00:32:03,540
So there are many different places and times we can flip a coin.

229
00:32:03,810 --> 00:32:20,610
We only ever get heads or tails out of all of them. So we typically will get the same outcome for many different states.

230
00:32:22,420 --> 00:32:23,530
In the state space,

231
00:32:25,120 --> 00:32:36,130
and the mathematical way of writing that is that this experiment X acting on states I can be equal to the experiment outcome for state J,

232
00:32:37,300 --> 00:32:43,870
when those states themselves are distinct elements in the set of all states Omega.

233
00:32:47,470 --> 00:32:51,760
We also have from our construction of the outcome space. The.

234
00:32:51,910 --> 00:32:59,260
So one thing I'll say is that the labels and indices on omega and X do not necessarily line up.

235
00:32:59,260 --> 00:33:04,300
So it's not necessarily the case that omega two gives x two and so on.

236
00:33:04,330 --> 00:33:14,140
These are different indices. Um we have constructed the outcome space so that all of the outcomes that we have

237
00:33:14,410 --> 00:33:20,950
with the index going to the smaller lowercase n are distinct by construction.

238
00:33:22,480 --> 00:33:26,740
So we don't need to worry about complications with probabilities.

239
00:33:27,370 --> 00:33:41,800
If we look at the probability of any two outcomes in this set, we will always have the sum of the probabilities acting on those individual outcomes.

240
00:33:42,190 --> 00:33:55,090
And just as a bit of notation, I can call those p and pj with at least the condition that um, I does not equal j to keep these outcomes distinct.

241
00:33:56,980 --> 00:34:11,360
So if we go back to. Our example of rolling a die and, uh, further specify that our die is fair and not rigged or weighted in some way.

242
00:34:13,350 --> 00:34:23,910
And you can probably tell me the the probabilities for the six elements in the outcome space.

243
00:34:24,510 --> 00:34:32,440
Um. The. This terminology of fear mathematically is the statement that probability for.

244
00:34:35,160 --> 00:34:38,270
Every side, every different side to come up is the same.

245
00:34:38,280 --> 00:34:45,450
So there's no preference or bias toward ruling a smaller or larger number or an even or odd number.

246
00:34:46,920 --> 00:34:51,690
So I can ask you now just check in.

247
00:34:51,690 --> 00:34:54,810
What is this joint probability p.

248
00:34:55,350 --> 00:35:01,080
That is the probability for getting any side from a fair rolling a fair die.

249
00:35:02,520 --> 00:35:06,600
Number six. It is indeed so.

250
00:35:08,620 --> 00:35:10,930
P equals one over six in the formal way.

251
00:35:11,800 --> 00:35:23,050
We can derive that if we did not know it in advance, is to do to select our event space to be the outcome space.

252
00:35:23,290 --> 00:35:27,370
Six events mutually exclusive. One, two, three, four, 5 or 6.

253
00:35:28,690 --> 00:35:34,630
The probability then is the sum from 1 to 6 of all pi.

254
00:35:35,080 --> 00:35:45,280
Since those are all equal, it is just Â£0.06. And our second condition on the probability definition is that that has to be equal to one.

255
00:35:45,940 --> 00:35:51,280
So that is how we do things. More generally, if we.

256
00:35:54,260 --> 00:36:05,899
Look at something slightly different for those four coin flips, and now also specifying that each of these coins is fair.

257
00:36:05,900 --> 00:36:09,110
So there's an equal probability of heads or tails.

258
00:36:10,070 --> 00:36:22,860
So. Just as with the with rolling the die, we can go back, look at the number of elements in the outcome space,

259
00:36:23,190 --> 00:36:30,570
and immediately say that the probability of each of those is one over 16.

260
00:36:31,860 --> 00:36:37,980
Here we can generalise things slightly by constructing a different sort of event space.

261
00:36:40,070 --> 00:36:50,630
Where we can ask what is the one event of interest is where we do these four coin flips and get an equal number of heads or tails.

262
00:36:51,440 --> 00:36:54,530
So we have a tie between those two possibilities.

263
00:36:55,430 --> 00:37:00,530
We can also consider, say, a different number of heads or tails.

264
00:37:02,820 --> 00:37:06,120
And maybe I'll ask you, what are the.

265
00:37:06,630 --> 00:37:10,230
We can have probabilities for both of these.

266
00:37:12,680 --> 00:37:30,980
And I can ask you what what they are. Go ahead and read it for us.

267
00:37:32,090 --> 00:37:36,220
Close. But three. That's the one.

268
00:37:36,790 --> 00:37:43,269
Um, and here, there's not really much shortcut beyond just counting the number of possibilities and

269
00:37:43,270 --> 00:37:50,080
applying the result that each individual element of A has this probability of 1/16.

270
00:37:51,280 --> 00:38:01,300
So for p equal we want to consider the probability of all ways of getting equal numbers.

271
00:38:03,250 --> 00:38:10,460
So. Just counting how many those are or how many there are of those.

272
00:38:12,050 --> 00:38:15,590
So we have three that have a heads first.

273
00:38:16,130 --> 00:38:20,210
If we have tails first then we also have those other three.

274
00:38:22,910 --> 00:38:30,520
So that is a total of. Six of the elements or there are six possibilities.

275
00:38:30,850 --> 00:38:39,550
Once we break up this probability into the sum of the individual four coin flip outcomes, those each have one sixteenths probability,

276
00:38:39,850 --> 00:38:47,380
and that simplifies to 3/8 for the probability of having a different set of heads or tails.

277
00:38:47,470 --> 00:38:48,910
Here we can use a shortcut.

278
00:38:49,360 --> 00:38:57,850
We can identify that equal a different are not only mutually exclusive, but also cover the complete set of possible outcomes.

279
00:38:58,360 --> 00:39:02,830
We will either have an equal number or a different number. There are no further possibilities.

280
00:39:04,150 --> 00:39:12,070
So using again the requirement that the probabilities of the full set of outcomes add up to 100%.

281
00:39:13,650 --> 00:39:23,219
This is a shortcut that we can take to get the 5/8 for having different possibilities,

282
00:39:23,220 --> 00:39:30,030
from the 3/8 of having equal possibilities without having to do that sort of counting.

283
00:39:32,430 --> 00:39:40,590
So what these examples are doing are assigning probabilities to events.

284
00:39:40,620 --> 00:39:45,750
Now that both probability and event have been formally mathematically defined.

285
00:39:46,320 --> 00:39:51,510
And I don't know if this is so much a definition or just a bit of terminology that.

286
00:39:53,520 --> 00:39:57,990
This is what we will mean when we talk about modelling.

287
00:39:59,070 --> 00:40:12,330
This is the process of assigning probabilities to events in our event space of interest that we select.

288
00:40:13,230 --> 00:40:24,030
So in the examples above, all of these probabilities are fixed by what we can call the symmetries of the system.

289
00:40:24,420 --> 00:40:34,380
So our assumption or requirement that the di or the coin flips are all fair introduces a symmetry between the different possible options.

290
00:40:35,850 --> 00:40:38,880
Um, so that's in the examples above.

291
00:40:40,320 --> 00:40:47,490
Things get more interesting and more complicated when we don't have symmetries that will completely fix probabilities,

292
00:40:48,030 --> 00:40:51,090
or if they exist, we don't necessarily know them.

293
00:40:51,870 --> 00:41:04,380
So a more general. Approach to this assignment of probabilities to events is to take.

294
00:41:08,000 --> 00:41:16,430
What we can call a data driven approach to basically get a lot of data and use that data to infer probabilities.

295
00:41:16,430 --> 00:41:22,670
So we repeat our experiments and our measurements many times.

296
00:41:26,730 --> 00:41:32,010
With each repetition of the experiment, we monitor the outcome.

297
00:41:34,160 --> 00:41:43,880
Exci that we get compared to the whole outcome space, which is also something that we might not know a priori, depending on the experiment.

298
00:41:44,450 --> 00:41:51,590
And then from having lots of data about the possible outcomes that we get and with which frequency and proportion.

299
00:41:53,690 --> 00:42:03,790
We can then infer the underlying probabilities that characterise or even define the probability space.

300
00:42:03,800 --> 00:42:16,610
So they are in our mathematical perspective and have, uh, eternal underlying quantities that we just don't know a priori and want to figure out.

301
00:42:17,150 --> 00:42:26,870
And the fact that we can actually do this and infer probabilities from past experiments is if we try to be,

302
00:42:27,270 --> 00:42:34,160
you know, very open minded and philosophically speculative is not necessarily something that we could guarantee.

303
00:42:34,760 --> 00:42:43,220
And this law of large numbers is what provides justification for taking this approach.

304
00:42:46,340 --> 00:42:54,340
I will say that. You know, a lot of what we are doing today is just stuff.

305
00:42:57,330 --> 00:43:05,070
Restating things that are probably intuitively obvious to many of you in this kind of more formal mathematical language.

306
00:43:05,610 --> 00:43:13,490
Um. So that this is a bit of an outlier for how this module is going to go.

307
00:43:13,520 --> 00:43:17,330
It won't be all like this sort of abstract, formal mathematical thing.

308
00:43:18,200 --> 00:43:23,390
And we will start to see that soon as we kind of get more.

309
00:43:25,600 --> 00:43:29,560
Uh, concrete, a more physical rather than mathematical.

310
00:43:29,740 --> 00:43:37,360
Um, so we are now sort of done with these foundational definitions,

311
00:43:37,360 --> 00:43:43,930
and we'll be going into some of these tools, like the law of large number and central limit theorem.

312
00:43:48,070 --> 00:43:50,559
So the tone will start to change.

313
00:43:50,560 --> 00:43:58,300
And especially when we talk about random walks, it will be a bit more like the, uh, the default for the remainder of the term.

314
00:43:58,810 --> 00:44:12,370
So if we choose to look at this law of large numbers and see how this justifies our approach to data driven modelling to infer probabilities,

315
00:44:13,450 --> 00:44:20,620
we can and without loss of generality, go back to this generic finite.

316
00:44:21,580 --> 00:44:28,300
Outcome space that has just the lowercase n elements in it.

317
00:44:31,330 --> 00:44:37,209
And introduce some slightly different notation that if we sum over all of the

318
00:44:37,210 --> 00:44:42,610
elements in this outcome space of the probability for that corresponding element,

319
00:44:43,180 --> 00:44:52,750
that is the same sum that we had earlier in a different, more cumbersome notation that just adds up to 100%.

320
00:44:54,550 --> 00:45:06,820
And the law of large numbers is going to deal with on one side, um, something that is called the expectation value for this probability space.

321
00:45:07,450 --> 00:45:18,700
So. The expectation value is something more physical that we will be talking about a great deal as we go forward,

322
00:45:19,000 --> 00:45:23,020
and it reflects the fact that we're not necessarily interested so much in the

323
00:45:23,020 --> 00:45:28,240
individual outcome of each separate experiment measurement that we carry out,

324
00:45:28,720 --> 00:45:32,830
but we want to somehow look at the collective behaviour,

325
00:45:32,830 --> 00:45:40,480
the collective features of lots and lots of these experiments, measurements and particles altogether.

326
00:45:40,930 --> 00:45:50,110
So in full generality, the expectation value of any arbitrary function that depends on these measurement

327
00:45:50,110 --> 00:45:55,660
outcomes is going to be denoted by these angle brackets around that function.

328
00:45:56,260 --> 00:46:05,890
And with our new notation and the assumption of a finite set that we can, um,

329
00:46:06,040 --> 00:46:19,000
write as a nice discrete sum sum over the entire outcome space of the value of this function for the outcome in the space,

330
00:46:20,290 --> 00:46:26,050
weighted by probability that we have for that outcome.

331
00:46:27,280 --> 00:46:30,840
So. The.

332
00:46:32,850 --> 00:46:38,460
Well. The simplest expectation value is when our function x is just one.

333
00:46:39,300 --> 00:46:44,100
Then we are just summing over the probabilities and get back to one at the end.

334
00:46:44,640 --> 00:46:49,170
So the expectation value of one is one. That's the kind of trivial example.

335
00:46:49,980 --> 00:47:01,680
Something that may still be non-trivial, but familiar is what's called the mean, which is the mean of the probability space itself.

336
00:47:03,330 --> 00:47:11,430
We'll call this the Greek letter mu that I've tried to draw there, hopefully obvious by context as well as my handwriting.

337
00:47:12,360 --> 00:47:18,150
This is the expectation value where our function of x just returns the outcome x itself,

338
00:47:18,750 --> 00:47:28,170
which is to say that we sum over all those outcomes of the outcome x weighted by the probability of getting that outcome.

339
00:47:29,070 --> 00:47:41,880
And that. Is going to be, you know, sort of simplest non-trivial collective behaviour for the probability space that we can really define.

340
00:47:44,700 --> 00:47:49,710
And say that, um, this expectation value, some people also call it the expected value.

341
00:47:50,310 --> 00:47:55,590
So this mu here is in a sense the expected outcome that we would get,

342
00:47:55,980 --> 00:48:02,580
even though it is possible for Mu itself not to be an element in the probability space.

343
00:48:02,610 --> 00:48:10,320
So for example, if we had heads and tails and we call that plus one and minus one and looked at the mean, you would get zero.

344
00:48:10,950 --> 00:48:19,950
It is not an element in the outcome space, but it is the average in between, equally distant from all of the possibilities that we have.

345
00:48:21,750 --> 00:48:29,820
What is useful to supplement the mean? And the last thing that I subject you to before giving you a chance for a brief break,

346
00:48:30,540 --> 00:48:42,809
is how much the outcomes in the probability space fluctuate around this mean or expected value that is related to the variance,

347
00:48:42,810 --> 00:48:48,720
again, of the probability space. This goes by the standard symbol sigma squared.

348
00:48:49,230 --> 00:48:53,940
Also something that many of you have likely seen in previous modules.

349
00:48:54,540 --> 00:49:09,970
So it is the expectation value. Of how much the outcomes differ from the mean with a squared there to ensure that, um, we could look at the,

350
00:49:10,990 --> 00:49:16,260
the magnitude, the absolute value of these fluctuations rather than having cancellations.

351
00:49:16,260 --> 00:49:23,670
So this is a measure of the magnitude or the scale of fluctuations around that mean.

352
00:49:24,030 --> 00:49:30,570
So putting it into the expectation value definition summing over all x in a,

353
00:49:32,430 --> 00:49:39,839
we have this function x minus mu squared, weighted by the probability of p of x.

354
00:49:39,840 --> 00:49:45,360
And we can plug in the fact that mu is itself an expectation value.

355
00:49:46,800 --> 00:49:55,350
So sigma squared. Here we have the difference between the individual x's and their expectation value squared.

356
00:49:55,980 --> 00:50:06,930
If we expand that square we have x squared minus two x and the expectation value plus expectation value squared.

357
00:50:06,930 --> 00:50:19,110
And all of that is an expectation value. I should have mentioned when I introduced the expectation value that it you can see it is a linear operation.

358
00:50:21,740 --> 00:50:29,220
You know, it is just a sum of whatever our function is, weighted by a number from 0 to 1.

359
00:50:30,330 --> 00:50:36,450
So the expectation value of a sum is the sum of expectation values.

360
00:50:36,990 --> 00:50:50,880
And we get here expectation value of the function x squared minus two times the expectation value of x multiplied by the expectation value of x.

361
00:50:51,900 --> 00:50:58,710
And then finally that expectation value squared itself is just a number for our probability space.

362
00:50:59,490 --> 00:51:03,330
The outer most expectation value does not affect that in any way.

363
00:51:04,650 --> 00:51:10,770
So expectation value of x squared is the same as expectation value times itself.

364
00:51:11,640 --> 00:51:14,190
And we end up with a result.

365
00:51:14,190 --> 00:51:24,930
You may have seen before elsewhere that the variance is the expectation value of the square minus the square of the expectation value.

366
00:51:26,930 --> 00:51:33,440
And related to this kind of trivially is the standard deviation.

367
00:51:34,010 --> 00:51:39,350
I will specify this is the standard deviation of the probability space.

368
00:51:40,140 --> 00:51:43,190
Since sometimes this terminology is used inconsistently.

369
00:51:44,240 --> 00:51:51,680
That's just one power of sigma of the square roots of sigma squared, and hence the square root of that.

370
00:51:54,250 --> 00:52:00,940
X squared minus square of x expectation values.

371
00:52:01,540 --> 00:52:06,790
So these are some of the basic quantities for collective behaviour.

372
00:52:07,150 --> 00:52:16,809
Get it back on the screen there for you to see. Um the basic stuff that we can start looking at to get collective behaviour out

373
00:52:16,810 --> 00:52:22,570
of a probability space with many different outcomes and degrees of freedom.

374
00:52:23,260 --> 00:52:30,670
I give one call for questions about this before, uh, ten minute break to re-energize.

375
00:52:31,150 --> 00:52:34,780
So it's 1153 and my clock.

376
00:52:34,780 --> 00:52:42,460
I'll start again at, uh, at 1203 and pause the recording, let you move around.

377
00:52:59,524 --> 00:53:07,294
I think this microphone will hopefully now function, and we'll see in the recording for anyone who wasn't able to make it today.

378
00:53:07,744 --> 00:53:12,214
If there's a difference equality between the first and second half of this.

379
00:53:14,724 --> 00:53:21,144
They're not seeing any immediate questions. Let's go back to the assignment of probabilities to events.

380
00:53:21,804 --> 00:53:27,203
That is modelling and is sort of the the theme for today,

381
00:53:27,204 --> 00:53:34,944
where one of the main ingredients in our general approach is to repeat an experiment many times.

382
00:53:35,304 --> 00:53:43,794
And we can interpret this in the framework of the probability foundations that we have now put in place.

383
00:53:46,304 --> 00:54:00,944
So. If we repeat our experiment several times this, we can actually interpret as a new kind of collective experiment, which just so.

384
00:54:03,794 --> 00:54:15,754
The new experiment just involves. Repeating our original experiment E and the corresponding measurement x some number of times.

385
00:54:15,764 --> 00:54:19,484
Let's call that r times four repetitions.

386
00:54:20,864 --> 00:54:26,774
And so this original experiment and measurement gave us our outcome space A.

387
00:54:27,614 --> 00:54:37,764
Let's continue with. The mild assumption that it is a finite outcome space with this lowercase n elements.

388
00:54:38,244 --> 00:54:45,204
Our new experiment, having repeated this some number of times, is giving us a new outcome space B.

389
00:54:48,964 --> 00:54:58,293
And we want to relate the properties and elements of this new outcome space be to the single experiment

390
00:54:58,294 --> 00:55:05,074
outcome space A and we have actually already done an example like this with those four coin flips.

391
00:55:05,614 --> 00:55:21,064
So if we. You think about specialising to the number of repetitions being for then elements in B will be combinations of four elements of A,

392
00:55:21,064 --> 00:55:32,433
so one reasonable element in this outcome space B, could be that we get the same outcome from our experiments each of the four times.

393
00:55:32,434 --> 00:55:35,614
So this is like flipping four coins and getting four heads.

394
00:55:36,124 --> 00:55:39,814
And then we have all of the other possibilities we already discussed.

395
00:55:40,414 --> 00:55:45,694
We could get the first outcome than the second outcome than the 99th outcome.

396
00:55:45,694 --> 00:55:52,084
And then back to the first outcome in these four repetitions of our experiment and so on.

397
00:55:53,644 --> 00:56:03,574
And we have also, in that simple example, gone through the consideration of how many elements we have in this new outcome space B.

398
00:56:08,834 --> 00:56:17,444
That for each repetition we have and possible outcomes coming from outcome space A so n

399
00:56:17,444 --> 00:56:23,984
possibilities for the first repetition multiplied by n for the second and for the third,

400
00:56:23,984 --> 00:56:37,484
and so on to however many repetitions that we have, which in general will be this and number of elements in outcome space A raised to the power here.

401
00:56:38,414 --> 00:56:43,664
In this example here, and with the four coin flips that are was just four.

402
00:56:44,084 --> 00:56:49,064
And we had in that case with n of two, two to the power 4 or 16 elements.

403
00:56:49,604 --> 00:57:02,084
And these powers will start um, helping the number of elements we have to consider increase very rapidly with the number of repetitions,

404
00:57:02,624 --> 00:57:06,794
uh, even with fairly modest single experiment outcome spaces.

405
00:57:08,384 --> 00:57:20,924
So what we have. In this setup is that because each element of B is built from R elements.

406
00:57:23,704 --> 00:57:38,763
Of a. We can call these are elements just x with an R in parentheses.

407
00:57:38,764 --> 00:57:46,804
To make it clear that that's not a power. This little R is a label that can run from one up to capital R.

408
00:57:47,044 --> 00:57:55,534
It is just the element of A from that set that we get in the lowercase r repetition going from one to capital r.

409
00:57:56,314 --> 00:58:05,784
So the consequence of this is that in the probability space that we construct for the repeated experiments based on b,

410
00:58:06,904 --> 00:58:12,514
the probability of any element in there x j I, k.

411
00:58:13,594 --> 00:58:16,864
However many repetitions that we have,

412
00:58:17,824 --> 00:58:29,914
we can decompose into the single experiment probabilities corresponding to that outcome space A for each of those individual elements.

413
00:58:31,624 --> 00:58:37,114
So X1 is going to be here what I've labelled x j.

414
00:58:37,534 --> 00:58:48,964
This gets multiplied by the probability for I, x k and so on for however many repetitions that we have.

415
00:58:51,694 --> 00:59:00,454
Some of the use of having this construction and this connection is that if we think about the average.

416
00:59:02,714 --> 00:59:13,354
Um element. So the average arithmetic mean let me define it for concreteness this will be x bar.

417
00:59:14,554 --> 00:59:18,874
So the average value of x that we get upon doing these are repetitions.

418
00:59:19,594 --> 00:59:23,824
So sum up the lowercase r from one to r.

419
00:59:24,484 --> 00:59:32,314
Measure the elements of a that we get each time, and then divide by r to take the average.

420
00:59:33,424 --> 00:59:48,344
This. Is something that we can analyse now as a random variable of the repeated experiment, as opposed to the the single experiment.

421
00:59:56,904 --> 01:00:08,544
And what this allows us to do is now relate this arithmetic mean to the mean that we defined again for the single experiment.

422
01:00:08,574 --> 01:00:16,854
So the simplest non-trivial characterisation of the collective behaviour of our probability space.

423
01:00:18,534 --> 01:00:22,754
So. This is our next task.

424
01:00:23,234 --> 01:00:27,614
You want to relate this arithmetic? Mean two.

425
01:00:31,174 --> 01:00:35,634
Sort of foundational mean mu hoops, which is equal to.

426
01:00:38,154 --> 01:00:44,424
The expectation value of any single one of these elements that we get from doing our experiments.

427
01:00:47,594 --> 01:00:54,744
And in this way you will. Establish the the law of large numbers.

428
01:00:55,104 --> 01:01:02,994
So the trick to to try to get a relation out of these is to consider, you know,

429
01:01:02,994 --> 01:01:07,074
we want some comparison of the arithmetic mean to the single experiment mean.

430
01:01:08,604 --> 01:01:12,024
The arithmetic mean is a random variable of the repeated experiment.

431
01:01:12,024 --> 01:01:16,614
So let's see how it fluctuates around this mu.

432
01:01:18,744 --> 01:01:33,374
And these fluctuations. Are related to the same construction of the variance that we looked at for just the single experiment.

433
01:01:33,384 --> 01:01:52,963
We now will generalise that. So looking at the expectation value here in the probability space corresponding to the repeated experiment,

434
01:01:52,964 --> 01:02:01,214
be of that difference square to give a measure of the absolute scale of these fluctuations.

435
01:02:03,654 --> 01:02:08,064
And thanks to our connection between the probability spaces of A and B,

436
01:02:08,514 --> 01:02:16,524
we can immediately re express these fluctuations in terms of the probability

437
01:02:16,524 --> 01:02:21,894
space of A and the individual measurement outcomes that we get from there.

438
01:02:22,734 --> 01:02:33,774
So we will have just our x r summed over r minus mu that's all squared.

439
01:02:34,044 --> 01:02:40,584
And then the expectation value that now is coming from the single experiment probability space A.

440
01:02:42,414 --> 01:02:47,034
So this is kind of a messy looking quantity.

441
01:02:47,124 --> 01:02:52,434
And we will now simplify. One way to simplify it is to recognise that.

442
01:02:54,974 --> 01:03:01,154
This mean? Mu is just a fixed number that characterises our single experiment probability space.

443
01:03:02,234 --> 01:03:08,864
If we imagine summing it up our times, we would just get our times mu.

444
01:03:09,554 --> 01:03:12,404
If we divide through by R, we just get mu itself back.

445
01:03:12,764 --> 01:03:21,494
And now we have both terms in this difference expressed with the same one over r and sum over r.

446
01:03:22,364 --> 01:03:28,694
So with our I recalling that the expectation value is a linear operation,

447
01:03:29,264 --> 01:03:36,674
that constant one over r factor can be pulled out of the square and out of the expectation value itself.

448
01:03:38,144 --> 01:03:45,284
And then we have just sums of experiment outcomes minus mu.

449
01:03:45,884 --> 01:03:53,114
And it's the whole sum that is squared rather than the individual terms within the sum.

450
01:03:54,074 --> 01:04:00,104
So I need to be careful with how the parentheses line up when going through this calculation.

451
01:04:02,024 --> 01:04:16,574
I will say one note is that formally, this little identity strictly only holds if that mu is finite and not divergent,

452
01:04:16,574 --> 01:04:25,694
and we will need to assume that both the mean and variance for the single experiment probability space are finite numbers, which is.

453
01:04:28,154 --> 01:04:32,674
Important to write down there.

454
01:04:32,684 --> 01:04:45,194
So that allows us to take that trick. We can now. You know, a square of a sum we can rewrite just as the product of two sums.

455
01:04:45,674 --> 01:04:50,744
So x r minus mu. This r is just a label.

456
01:04:51,134 --> 01:04:57,044
So we can switch that label from our to s both independent.

457
01:04:59,354 --> 01:05:04,933
Indices going over the possible um outcomes that we get.

458
01:05:04,934 --> 01:05:12,974
So remember that the expectation value itself is a sum over all elements of A.

459
01:05:13,304 --> 01:05:18,584
So there are even more sums that we are writing down in here at this moment.

460
01:05:20,384 --> 01:05:25,124
For now, we can well recognise that if we have.

461
01:05:26,844 --> 01:05:33,414
Say two independent sums being multiplied together in a sub r and a b sub s,

462
01:05:34,224 --> 01:05:38,634
then expanding those into thinking about all the different terms that we have.

463
01:05:38,964 --> 01:05:48,114
We get all of the term by term products, and we can combine this into a single sum over all values of those two indices.

464
01:05:48,594 --> 01:05:52,434
That is just the product of a sub r and b sub s.

465
01:05:53,694 --> 01:06:08,124
So that is to say that we have another quantity that we can pull out of the expectation value as a linear operation that sum over r and S.

466
01:06:08,904 --> 01:06:11,063
And then the expectation value is.

467
01:06:11,064 --> 01:06:21,804
Now within this sum um we would need to be careful about this if we had not assumed that we had a finite number of elements in a,

468
01:06:22,014 --> 01:06:26,454
you know, change of the order of sums can be tricky. So that's why we did that.

469
01:06:26,454 --> 01:06:33,384
It does generalise, and that generalisation is kind of a technical thing that does not change the story.

470
01:06:33,384 --> 01:06:39,624
And I'll just say that it goes through rather than working in full generality here and now.

471
01:06:40,224 --> 01:06:55,404
But now we have the product of the two terms that we had formerly in the separate sums x r minus mu and x minus mu inside the expectation value.

472
01:06:56,184 --> 01:07:11,034
Let me. It's do a bit of separation between that aside, and the quantity that we have in here is also a result that we could prove in detail.

473
01:07:11,634 --> 01:07:22,884
I get it back on the screen. What to do about this sort of expectation value of a product of two fluctuations around the mean.

474
01:07:23,274 --> 01:07:27,744
Here each individual fluctuation is coming with a sign.

475
01:07:28,854 --> 01:07:30,894
Some will be above the mean, some will be below.

476
01:07:31,494 --> 01:07:40,853
And I'll go into physicist mode and just give a qualitative argument that if we have different experimental

477
01:07:40,854 --> 01:07:50,814
outcome results that are being summed over all of the elements in our single experiment outcome space,

478
01:07:51,324 --> 01:07:56,994
then we can rely on all of those positive and negative fluctuations coming in uncorrelated,

479
01:07:57,294 --> 01:08:04,404
cancelling out in the sum to an approximation that does become exact as the number of outcomes becomes large.

480
01:08:05,364 --> 01:08:13,884
So in general, when R and S are different experiment iterations, this expectation value will vanish.

481
01:08:15,114 --> 01:08:21,504
If instead we have the special case that is allowed by our some that's and are the same outcome, then they are correlated.

482
01:08:21,774 --> 01:08:28,344
We get back to just having a square and a magnitude that is non-negative and adds up.

483
01:08:29,124 --> 01:08:38,124
And in fact we can recognise it as exactly this variance of the single experiment x minus mu squared.

484
01:08:38,634 --> 01:08:42,594
So we will get out of here that variance sigma squared.

485
01:08:43,014 --> 01:08:47,244
When r and s are the same we will get zero. When are in S are different.

486
01:08:47,784 --> 01:08:55,554
And mathematical notation that summarises this is called the Kronecker delta function with these indices r and s.

487
01:08:56,154 --> 01:09:01,284
Um, have you seen Kronecker delta as before, you are nodding.

488
01:09:02,244 --> 01:09:06,084
There aren't any questions or confusions about what that itself means.

489
01:09:10,974 --> 01:09:17,513
Good. So I think this is the kind of single longest calculation we'll be doing today.

490
01:09:17,514 --> 01:09:22,584
And we are almost at the end of it. So going on to the next page.

491
01:09:24,294 --> 01:09:31,704
Everything now boils down to one over r squared times the sum over all of these

492
01:09:32,994 --> 01:09:40,254
repetitions of a single experiment of sigma squared times the Chronica delta r and s.

493
01:09:40,884 --> 01:09:44,454
As we've said, that is one when r equals s and zero otherwise.

494
01:09:44,874 --> 01:09:52,974
So that sum immediately simplifies with a sigma squared up top to just a sum over one of those variables.

495
01:09:53,574 --> 01:10:00,144
And the case when they are the same when we get one from the Kronecker delta and that is um capital r.

496
01:10:02,394 --> 01:10:12,114
Uh terms each of which one which adds up to an R that cancels off one of these factors in the denominator.

497
01:10:12,774 --> 01:10:25,554
And we end up just with sigma squared over r as a very simple result from the scale of these fluctuations of the experiment.

498
01:10:25,554 --> 01:10:28,884
Random variable around the single experiment mean.

499
01:10:31,034 --> 01:10:44,204
So, as we have assumed that the single experiment variance sigma squared itself is finite, we can say as our final conclusion of this,

500
01:10:44,204 --> 01:10:51,854
that in the limit where we take a very large number of repetitions, formally infinite,

501
01:10:53,084 --> 01:11:00,434
then I try to keep this up as much of it as I can fit on the screen for as long as I can.

502
01:11:01,004 --> 01:11:13,134
Um. We look at the difference between arithmetic mean and single experiment mean, that goes to zero a finite number over something going to infinity.

503
01:11:14,154 --> 01:11:19,643
And because we have set this up in such a way that every term in the expectation value,

504
01:11:19,644 --> 01:11:28,014
that is a sum over the outcome space is non-negative, either zero or positive.

505
01:11:28,584 --> 01:11:39,684
The only way to get an overall zero out of a sum of lots of non-negative terms is for every single one of them to be zero themselves.

506
01:11:40,344 --> 01:11:48,144
So. This is a fairly common trick you may have seen before.

507
01:11:48,354 --> 01:11:54,564
We have a sum of squares. If it vanishes, then every single term itself must vanish.

508
01:11:56,944 --> 01:12:06,424
And if we write down, well, any given term that is in here, it is just going to be we can just drop the expectation value.

509
01:12:07,414 --> 01:12:11,314
We have that in this limit of many repetitions.

510
01:12:11,824 --> 01:12:21,214
The arithmetic mean coming from the repeated experiment probability space exactly reproduces the single experiment mean mu.

511
01:12:21,724 --> 01:12:26,304
The simple characterisation of the basic probability space itself.

512
01:12:26,314 --> 01:12:39,734
And this. Is the law of large numbers that I mentioned as the justification for an approach to data driven modelling.

513
01:12:40,544 --> 01:12:45,674
It tells us that if we repeat an experiment many times and monitor the outcomes,

514
01:12:47,234 --> 01:12:53,674
the arithmetic means that we get out of that monitoring are just going to produce.

515
01:12:53,684 --> 01:13:03,434
They're going to give us the underlying characteristics of the single experiment probability space that we were after to start off with,

516
01:13:03,434 --> 01:13:06,614
to assign probabilities to events in generality.

517
01:13:07,394 --> 01:13:21,014
And we can also generalise this to say that for sufficiently large R this arithmetic mean is approaching the true exact mean.

518
01:13:21,644 --> 01:13:30,673
So for large r c much greater than one, we will have um,

519
01:13:30,674 --> 01:13:35,624
an approximate relation that becomes better and better systematically as r increases and we get

520
01:13:35,624 --> 01:13:42,044
closer and closer to the underlying probability distribution that is governing our experiments.

521
01:13:42,734 --> 01:13:54,154
And a more like qualitative, conceptual or physical way of saying this is that from many different particles, many different degrees of freedom,

522
01:13:54,164 --> 01:14:01,754
many different repetitions, what we are getting is smooth behaviour that is governed by the underlying probability space,

523
01:14:02,474 --> 01:14:07,814
rather than what we might have worried about if we had not gone through this exercise

524
01:14:07,814 --> 01:14:13,543
of those probabilities piling up or giving very wildly fluctuating behaviour,

525
01:14:13,544 --> 01:14:17,624
was a lot of them were being superimposed. That doesn't happen.

526
01:14:17,954 --> 01:14:29,263
We get to the underlying characteristics of our probability space that we, um, that was our starting point.

527
01:14:29,264 --> 01:14:33,874
And that's the kind of thing that we will want to construct in more realistic situations.

528
01:14:33,944 --> 01:14:39,964
So kind of a long derivation for a simple and possibly familiar result.

529
01:14:39,974 --> 01:14:50,334
Any questions about that? I should keep an eye on the time now.

530
01:14:50,364 --> 01:14:59,274
There's plenty of time. So that is one of the tools that we are introducing today.

531
01:15:00,384 --> 01:15:03,924
Getting a bit less formal now. The other one is the central limit theorem.

532
01:15:04,674 --> 01:15:10,913
And. Central.

533
01:15:10,914 --> 01:15:18,194
The discussion of the central limit theorem I'm going to do now is going to be even less formal than what we've gone through for the law.

534
01:15:18,234 --> 01:15:24,174
Law of large numbers. Um, I suspect it's something you have seen before.

535
01:15:24,594 --> 01:15:30,354
And I will say there are lots of different ways of phrasing it or formulating it, and we're applying it.

536
01:15:30,354 --> 01:15:40,344
So primarily we will just go through today and state the way that we're going to be applying it for statistical mechanics going forward.

537
01:15:41,154 --> 01:15:45,803
The one thing we have to do, uh, in order to, uh,

538
01:15:45,804 --> 01:15:55,343
have that be a self-contained statement is to drop our assumption of having a

539
01:15:55,344 --> 01:16:03,144
finite outcome space and generalise to the infinitely uncountable situation.

540
01:16:04,584 --> 01:16:15,834
So generalising to a measurement X that produces some continuous value and

541
01:16:15,834 --> 01:16:22,464
gives us an outcome space that is not merely infinite but in fact uncountable.

542
01:16:23,484 --> 01:16:32,334
This doesn't have to be an elaborate measurement. It could be measuring a position that can give any real axis, any real number on the real axis.

543
01:16:32,784 --> 01:16:42,144
You know, assuming that that measurement can be done to infinite precision gives us the real numbers as our outcome space in that case here.

544
01:16:44,304 --> 01:16:53,363
Instead of having individual probabilities for each outcome that we some the way that we,

545
01:16:53,364 --> 01:17:00,174
uh, looked at earlier when introducing the expectation value, that is.

546
01:17:02,334 --> 01:17:04,974
Insufficient for uncountable numbers.

547
01:17:05,364 --> 01:17:17,034
And to get a probability in this case, we need to generalise to a distribution or density of these probabilities and then integrate over that.

548
01:17:25,564 --> 01:17:29,704
So I'll be calling this a, uh, probability distribution.

549
01:17:32,624 --> 01:17:41,204
Sometimes called a probability density, and the probability, for example, of our measurement outcome.

550
01:17:42,074 --> 01:17:52,544
In the continuous case where it is a real number, we have to set this up as that real number being between some endpoints and those endpoints.

551
01:17:53,204 --> 01:18:00,044
Go into the integral over this lower case p of x with respect to d x,

552
01:18:00,044 --> 01:18:07,034
where this x now is our continuous measurement outcome in the within the real numbers.

553
01:18:08,744 --> 01:18:19,184
So. Conceptually, you know, that's a change that took Leibnitz, Newton to introduce many years ago.

554
01:18:19,514 --> 01:18:25,904
But functionally, all of the concepts that we have introduced already will pass over fairly smoothly,

555
01:18:26,264 --> 01:18:32,324
so long as we are careful about the contrast between distributions and probabilities.

556
01:18:32,954 --> 01:18:41,834
So just like a individual probability, the expectation values will also become integrals.

557
01:18:49,644 --> 01:19:01,374
And here the expectation value, uh, which we introduced first as a discrete sum, because that was going over all of the outcomes in that finite set A,

558
01:19:02,964 --> 01:19:14,424
the integrals that we have to consider for expectation values given in uncountable outcomes, set a need to be taken over the entire domain.

559
01:19:15,594 --> 01:19:29,064
Wherever we have a non-zero probability distribution p of x, we would have the expectation value of any function of our possible outcomes going,

560
01:19:29,064 --> 01:19:36,714
say from minus infinity to infinity of that function weighted by the probability distribution,

561
01:19:38,514 --> 01:19:47,394
and I may well omit those limits minus infinity to infinity going forward just for, uh, speed and simplicity.

562
01:19:47,874 --> 01:20:00,644
So now if I were to do that for the simplest expectation value, f equals one for any x, we would have just the integral of one times p of x dx.

563
01:20:01,524 --> 01:20:10,103
And our requirements for the probabilities to make sense tell us that integrating the distribution over the

564
01:20:10,104 --> 01:20:17,484
full domain has to give us the total 100% probability something has to happen within the domain of outcomes.

565
01:20:19,404 --> 01:20:32,604
So more generally, or less trivially, we can write down the expectation value of the outcome x raised to any integer power l.

566
01:20:33,384 --> 01:20:44,844
This is just the integral of the probability distribution p of x multiplied by x to the l when we set L to one,

567
01:20:45,924 --> 01:20:53,814
then we get back to the mean the expectation value of x that we saw before.

568
01:20:54,354 --> 01:20:59,214
If we want, then the variance. That was the, uh.

569
01:21:01,624 --> 01:21:06,154
Expectation value of the square. Minus the square of the mean.

570
01:21:06,514 --> 01:21:12,944
Then we have that mean already from setting L equal one expectation value of the square.

571
01:21:12,964 --> 01:21:24,064
Just sets L equal to. And that will be a good starting point for um working with continuous probability spaces.

572
01:21:26,854 --> 01:21:28,384
Questions about that quickly.

573
01:21:34,844 --> 01:21:48,494
Not too much changes, but we now have the ingredients that we need to state the version of the central limit theorem that we will use for this module.

574
01:21:48,494 --> 01:21:54,434
And in particular you will use in the computer assignment know over the next few weeks.

575
01:21:58,064 --> 01:22:11,414
So I can refer to that as a CLT. So our setup is that we have some number of random variables.

576
01:22:12,044 --> 01:22:20,264
So any of them. Call them X1, x2 and so on up to x1.

577
01:22:21,974 --> 01:22:26,054
All of these random variables have the same.

578
01:22:28,304 --> 01:22:43,934
Mean and variance, which we will assume is will require is finite, so mu and sigma squared are identical for all x and also finite.

579
01:22:44,054 --> 01:22:50,004
This is exactly what we had when we repeated an experiment many times.

580
01:22:50,024 --> 01:22:54,253
Each repetition was a new random variable. It was the same experiment.

581
01:22:54,254 --> 01:22:59,774
So it produced that same, uh characteristics the mean and expectation value.

582
01:23:00,494 --> 01:23:15,404
I will say if you have done more formal or in-depth statistics modules, this is sometimes called i.i.d. variables that arises quite commonly.

583
01:23:16,394 --> 01:23:21,164
I.I.D. stands for identical and identically distributed.

584
01:23:26,064 --> 01:23:35,423
And in fact, you know, these sorts of variables are so common that that is why the central limit theorem has the name that it does.

585
01:23:35,424 --> 01:23:42,294
It is a theorem that is central to, uh, statistics and stochastics in general.

586
01:23:43,434 --> 01:23:46,464
And question, uh, to independence.

587
01:23:46,704 --> 01:23:54,204
Um, yes it is. So sorry to the danger of talking and writing at the same time.

588
01:23:54,834 --> 01:24:07,824
Um, yes. Lupin. Independent and identically distributed, identical and identically distributed is a bit redundant.

589
01:24:08,424 --> 01:24:11,444
So you have seen this before, or at least some of you have.

590
01:24:11,454 --> 01:24:18,384
If you have not, then um, don't worry, we have seen it already today with our example of that repeated experiment.

591
01:24:19,044 --> 01:24:27,294
And similarly to considering the arithmetic mean as a random variable of that repeated experiment,

592
01:24:28,224 --> 01:24:33,234
we now consider a similar collective random variable,

593
01:24:33,234 --> 01:24:43,433
which will just be the sum rather than the average of all of these i.i.d., um, individual random variables.

594
01:24:43,434 --> 01:24:47,304
So taking the sum from one to n,

595
01:24:47,934 --> 01:24:57,114
so the single experiment in the language you were using before now has the subscript the repeated experiments, the sum does not.

596
01:24:57,834 --> 01:25:05,664
It may not be the, the best notation, um, but hopefully it will be clear enough.

597
01:25:06,924 --> 01:25:16,584
So shout out any questions if you lose track of what quantities correspond to, um, what situations or what meanings.

598
01:25:17,394 --> 01:25:21,654
So that some we have as a.

599
01:25:24,514 --> 01:25:28,354
Collective random variable and the statement of the central limit theorem.

600
01:25:28,354 --> 01:25:39,484
Finally. Which I will put just on a new page, so we'll have plenty of space and not have to worry about squeezing things in.

601
01:25:43,024 --> 01:25:50,884
Just states that when we have a large number of i.i.d. variables,

602
01:25:51,544 --> 01:26:02,854
the probability distribution that governs our collective random variable as the sum is given by a Gaussian form.

603
01:26:03,544 --> 01:26:08,464
This is a continuous distribution over the real numbers.

604
01:26:09,124 --> 01:26:12,484
So same p of x we had before.

605
01:26:18,034 --> 01:26:21,514
In Gaussian integrals. I think we talked about on Monday.

606
01:26:25,054 --> 01:26:30,724
They always have this exponential factor with an x squared in there.

607
01:26:31,234 --> 01:26:43,504
And here we will look at the fluctuations of this collective variable x around n times the mean of the single experiment.

608
01:26:45,544 --> 01:26:57,344
Independent variables. This is also normalised by two n times the variance of that single experiment.

609
01:26:57,364 --> 01:27:02,314
Each individual random variable has that same finite sigma squared.

610
01:27:03,004 --> 01:27:11,614
And then, in order to have a well-defined probability when we integrate this, we need a pre factor out front that.

611
01:27:12,184 --> 01:27:18,274
I think there's this extra practice problem that you can do,

612
01:27:18,274 --> 01:27:27,003
if you are so inclined to show that the variance also appears in this constant pre factor us exactly.

613
01:27:27,004 --> 01:27:33,724
In order for the integral of the probability distribution to add up to 100%.

614
01:27:35,074 --> 01:27:44,463
And the. Key thing about this conceptually is that the the collective behaviour of our large number

615
01:27:44,464 --> 01:27:53,394
of degrees of freedom is being dictated entirely by properties of the single experiment,

616
01:27:53,404 --> 01:27:57,843
the single variable that we introduce, and in particular,

617
01:27:57,844 --> 01:28:09,244
just the mean and the variance of that single experiment are controlling the probability distribution of the collective situation.

618
01:28:09,904 --> 01:28:13,654
So we can say that the collective behaviour.

619
01:28:16,054 --> 01:28:20,074
Of these many particles. In more physics language.

620
01:28:22,964 --> 01:28:30,224
Is being governed by just the. Mean and variance of.

621
01:28:34,724 --> 01:28:41,504
A single particle system, or the experiment in measurement that we started off with.

622
01:28:43,364 --> 01:28:52,544
And this is, you know, just to tie it to some of the big themes of the module showing how we we can.

623
01:28:54,664 --> 01:28:59,314
Yet smooth, robust, reliable, reproducible collective behaviour.

624
01:28:59,944 --> 01:29:06,874
Even when we consider not solving the the large and system exactly,

625
01:29:06,874 --> 01:29:17,884
but relying on probabilistic sampling um in order to to carry out mathematical analyses of this.

626
01:29:18,514 --> 01:29:26,494
So. One question that we do have some time to consider here is, uh.

627
01:29:30,534 --> 01:29:37,404
Just the statements. You know, there is possibly ambiguity about what and much greater than one really means.

628
01:29:37,404 --> 01:29:47,694
How large is a large n where this probability distribution coming from the central limit theorem actually becomes a reasonable approximation.

629
01:29:48,624 --> 01:29:55,074
And really I notice I should have used an approximately equals to sign there,

630
01:29:55,224 --> 01:30:05,214
which um becomes an exact equality in that limit that n goes to infinity.

631
01:30:06,744 --> 01:30:07,973
So I was a bit sloppy there.

632
01:30:07,974 --> 01:30:18,383
But we will see that even for values of n that are not necessarily very large, we will get good approximations, at least in some simple cases.

633
01:30:18,384 --> 01:30:28,554
So this is just a visual illustration that is easier to to look at compared to be sketching it by hand.

634
01:30:29,724 --> 01:30:33,203
This is something we had earlier rolling a six sided die. That is fair.

635
01:30:33,204 --> 01:30:43,374
Roll it. Once we get one sixth for every, uh, outcome in the outcome space of every different side of the die coming up on top.

636
01:30:43,914 --> 01:30:47,874
So that's that horizontal variable k is just that number.

637
01:30:48,444 --> 01:30:54,054
And then if we imagine repeating that experiment again and adding up the numbers that we get each time,

638
01:30:54,534 --> 01:31:02,333
we have a minimum of two, a maximum of 12, and various different combinations that can add up to that same number.

639
01:31:02,334 --> 01:31:11,694
So similarly for three or 4 or 5 repetitions, everything is behaving as we would expect,

640
01:31:12,204 --> 01:31:19,164
as we consider that some of n random variables given by rolls of a die.

641
01:31:19,584 --> 01:31:25,964
So by the time we have five rolls, we can get up to 30 as our possible total.

642
01:31:25,974 --> 01:31:34,404
The bare minimum is five. And we can see that this histogram of the possible outcomes that we have.

643
01:31:34,404 --> 01:31:41,454
So just counting the number of times we get any particular sum after rolling the die five times

644
01:31:41,994 --> 01:31:48,834
is starting to look like the bell curve that we can think of as a Gaussian distribution.

645
01:31:49,704 --> 01:32:01,884
So what's done in the final plot here is actually to, uh, reproduce all of these five histograms with the coloured lines,

646
01:32:02,274 --> 01:32:09,084
which might be a bit challenging to see with the sun coming into the position that it is.

647
01:32:09,714 --> 01:32:15,564
So I'll say that this blue line at the top is the constant and equal one case.

648
01:32:17,554 --> 01:32:23,064
And then we have n equals two for the linear green line coming from there.

649
01:32:23,544 --> 01:32:28,914
We have normalised all of these lines so that they have the same maximum in the middle.

650
01:32:31,074 --> 01:32:37,134
We similarly look at n equals three four and five in orange red and blue.

651
01:32:37,794 --> 01:32:46,584
And then the black curve is the Gaussian prediction coming from the central limit theorem.

652
01:32:47,994 --> 01:32:56,274
And we can see that by the time we have just five rolls of the die with six possible outcomes in the outcome space.

653
01:32:56,874 --> 01:33:05,364
So both modest numbers, we are clearly approaching that black curve governed by the central limit theorem.

654
01:33:05,934 --> 01:33:16,704
And we should have a chance in the tutorial tomorrow to generalise this and look at a some more interesting situation.

655
01:33:16,704 --> 01:33:24,324
And uh, another example of the power of the central limit theorem, even when this n is.

656
01:33:27,644 --> 01:33:36,694
Not necessarily as. Far above one as we might have to expect in statistical mechanics or thermodynamics.

657
01:33:36,704 --> 01:33:41,113
Of course, our base number is Avogadro's number.

658
01:33:41,114 --> 01:33:45,644
We will typically be thinking about something with ten to the power 22.

659
01:33:46,184 --> 01:33:56,204
And if the central limit theorem is already giving us reasonable expectations for six outcomes repeated five times,

660
01:33:56,624 --> 01:34:01,994
we can imagine and appreciate just how well it is going to work.

661
01:34:02,594 --> 01:34:15,254
Once we have 1000 trillion trillion atoms to to worry about in our, uh, big statistical mechanics system.

662
01:34:18,184 --> 01:34:30,304
Five minutes left for today. Questions before we probably take a look at what we will be doing next week and introduce the concept of random walks.

663
01:34:33,684 --> 01:34:44,964
So no questions so far. And we will look at random walks as as an application of that central limit theorem.

664
01:34:45,414 --> 01:34:59,394
Though I should also say that. As a general concept, this is a very useful modelling tool that is applicable to a wide range of phenomena.

665
01:35:00,534 --> 01:35:06,654
More than you might naively guess if you haven't seen it before.

666
01:35:06,654 --> 01:35:22,014
So random walks in general are a tool to model various physical or theoretical systems and get probabilities associated to outcomes.

667
01:35:22,464 --> 01:35:29,004
So some of the obvious examples are things where there is actually an object that is walking randomly.

668
01:35:29,874 --> 01:35:42,114
Brownian motion. I mentioned on Monday that the random fluctuations of a small pollen grain or similar object

669
01:35:42,474 --> 01:35:47,964
that is suspended in a liquid and constantly being bombarded by the particles in that fluid.

670
01:35:48,504 --> 01:35:52,374
It ends up walking randomly within that space.

671
01:35:52,944 --> 01:36:03,114
But we can also think more abstractly about, say, stock prices as carrying out random walks in a space of money.

672
01:36:03,534 --> 01:36:07,614
So the value of a stock can rise or can fall with some elements of randomness.

673
01:36:08,664 --> 01:36:15,654
Um, there's been a lot of progress in evolutionary biology,

674
01:36:15,984 --> 01:36:27,234
in modelling genetic drift in DNA as a random walk process that can then give you estimates for how closely two different species are related,

675
01:36:27,504 --> 01:36:33,624
based on how many steps it seems that their DNA would have to take to get from one to the other.

676
01:36:33,624 --> 01:36:40,554
So how long ago did that DNA start to diverge, uh, on the basis of random mutations, that recombination.

677
01:36:41,334 --> 01:36:47,544
So the basic idea, which is all we will do today, is kind of what it says on the tin.

678
01:36:48,764 --> 01:36:55,674
We think about an object, a walker, that takes a sequence of random steps.

679
01:36:57,204 --> 01:37:06,023
So with each step, the current state of the walker in general, whether that is a physical position,

680
01:37:06,024 --> 01:37:13,194
a price, or a DNA sequence, we'll have some small update to a new state.

681
01:37:16,714 --> 01:37:21,634
And we repeat this as usual. Many times.

682
01:37:25,304 --> 01:37:31,664
And look at how the object moves on the basis of these repeated random steps.

683
01:37:35,604 --> 01:37:39,204
So we will do an example in detail.

684
01:37:39,694 --> 01:37:44,454
Um, on Monday after the after tomorrow's tutorial.

685
01:37:45,174 --> 01:37:58,044
Here I will just say that this is kind of a, um, particular example of a general mathematical concept known as a Markov process.

686
01:37:59,574 --> 01:38:08,153
Um, a Markov process is exactly the situation where we look at a sequence of states with the

687
01:38:08,154 --> 01:38:15,593
condition that each new state in that sequence depends only on the state that came before it,

688
01:38:15,594 --> 01:38:23,154
plus some random update. So there's no memory that the system maintains of the whole chain that came before.

689
01:38:23,574 --> 01:38:28,254
That's all lost and everything is reset fresh with each step.

690
01:38:29,694 --> 01:38:38,004
Um, and the sequence of states that are then produced in that way is sometimes called a Markov chain.

691
01:38:38,994 --> 01:38:43,824
You go from one state links to the next that is linked to the next, that is linked to the next.

692
01:38:44,214 --> 01:38:52,824
And there are no other loops or more complicated behaviour that is arising in the process of generating that chain of states.

693
01:38:54,144 --> 01:39:02,274
We will come back to this quite a bit, quite a bit further in the term, possibly after the term break,

694
01:39:02,814 --> 01:39:08,964
as a tool to do numerical analyses of statistical systems based on randomness.

695
01:39:09,354 --> 01:39:13,894
So more general than that, random walks, but a, uh,

696
01:39:14,424 --> 01:39:24,264
fairly powerful or very powerful underlying tool to numerically analyse systems with many degrees of freedom based on randomness.

697
01:39:24,274 --> 01:39:28,164
So I'll stop there with the idea of random walks.

698
01:39:28,584 --> 01:39:34,974
It will become more concrete with a detailed example next week, and all that's left for today,

699
01:39:34,974 --> 01:39:39,924
I think is a final call for questions before we break and reconvene.

700
01:39:40,644 --> 01:39:51,084
Also here, uh, tomorrow morning. Okay, I'll let you go.

