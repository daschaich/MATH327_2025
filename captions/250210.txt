1
00:00:02,590 --> 00:00:06,580
[Auto-generated transcript. Edits may have been applied for clarity.]
Okay. I think we are ready to get underway.

2
00:00:06,590 --> 00:00:13,600
I don't know how or why the lights changed, but I think it will help the projector show it more clearly.

3
00:00:14,020 --> 00:00:17,020
So we'll see how that goes.

4
00:00:17,020 --> 00:00:20,920
And if it changes back by the time we're back here on Wednesday.

5
00:00:25,770 --> 00:00:30,719
So one piece, one more piece of logistics before we get started.

6
00:00:30,720 --> 00:00:36,600
In addition to that six digit attendance code 126946.

7
00:00:37,710 --> 00:00:41,640
Um, we are now in week three. You can no longer drop this module.

8
00:00:42,570 --> 00:00:44,850
And to make sure that I'm kept honest.

9
00:00:45,480 --> 00:00:53,430
Uh, now that you're not free to come and go as you please anymore, we have the usual mini questionnaire up on canvas for you.

10
00:00:54,270 --> 00:01:00,150
Hopefully you are receiving the weekly announcements and can get the link to it from there.

11
00:01:00,840 --> 00:01:06,840
If not that, it is in the quizzes section along the left hand side of the canvas menu.

12
00:01:07,650 --> 00:01:11,700
It'll be active through the end of the week, uh, into the weekend.

13
00:01:12,030 --> 00:01:19,560
And do let me know of any issues or barriers that you are encountering so that we can, uh, deal with them.

14
00:01:21,570 --> 00:01:28,110
Uh, sooner rather than later, and, well, hopefully we will be able to deal with them.

15
00:01:28,110 --> 00:01:35,130
Unlike the projector that always has to be rebooted whenever I switch to a different window.

16
00:01:35,730 --> 00:01:44,490
So, any immediate questions before we recap where we are now that we're done with the probability foundations?

17
00:01:54,170 --> 00:02:06,200
So last week we did wrap up all of the random walks, lore of diffusion and connections to the Central Limit theorem,

18
00:02:07,190 --> 00:02:13,640
and then moved on to one of the central topics for this module going forward.

19
00:02:14,150 --> 00:02:20,959
So I'll review that maybe in a little more depth or detail than usual,

20
00:02:20,960 --> 00:02:27,950
because these are some big concepts that will be used repeatedly throughout the remainder of the course,

21
00:02:27,950 --> 00:02:34,999
based around statistical ensembles as the probability spaces that we can get.

22
00:02:35,000 --> 00:02:44,210
If we just think about a collection of degrees of freedom that we allow to evolve in time, and I'll look at them from time to time.

23
00:02:45,980 --> 00:02:54,260
And in particular, we don't try to trace the time evolution of all of the many particles that are going to be in these systems.

24
00:02:55,880 --> 00:03:02,660
We imagine that we are stochastic sampling the states of the system, the so-called micro states.

25
00:03:06,800 --> 00:03:10,010
But describe the microscopic degrees of freedom.

26
00:03:11,030 --> 00:03:19,760
Those are Omega I, and each omega I has its corresponding probability pi,

27
00:03:20,000 --> 00:03:26,120
which we do not know our priority, but does describe the system in full detail,

28
00:03:28,100 --> 00:03:36,920
subject to certain physical constraints, such as conservation of these particle numbers and conservation of energy.

29
00:03:37,700 --> 00:03:45,650
Those two together give us the first concrete statistical ensemble we will focus on,

30
00:03:47,480 --> 00:03:55,790
called the micro canonical ensemble, and characterised by the conservation of its internal energy and particle number.

31
00:03:57,710 --> 00:04:06,200
That conservation of energy also goes by the first law of thermodynamics, at least for closed or isolated systems,

32
00:04:06,740 --> 00:04:18,469
and a picture that we can have in mind for the micro canonical ensemble is this, which we will eventually contrast with other possibilities.

33
00:04:18,470 --> 00:04:22,310
We have the particles, the degrees of freedom in our system.

34
00:04:22,880 --> 00:04:28,520
They are locked into, uh, into place into some container.

35
00:04:29,180 --> 00:04:31,940
So the number of particles is not changing over time.

36
00:04:32,330 --> 00:04:41,540
And this container is insulated, isolated from the rest of the universe, so that no energy can pass through this barrier.

37
00:04:42,500 --> 00:04:48,680
And the energy is then directly conserved within this micro canonical system itself.

38
00:04:49,520 --> 00:04:56,509
This will prove awkward if we try to actually, you know, think about observing or analysing these systems.

39
00:04:56,510 --> 00:05:02,810
If we're not allowed to exchange photons with it or get any energy in or out, that will be an obstruction.

40
00:05:03,890 --> 00:05:09,290
But that's getting ahead of ourselves. We have still a few things to do with the micro canonical ensemble first.

41
00:05:10,130 --> 00:05:21,590
In particular, looking at the connection to our everyday experience of statistical systems like the air in this room dying.

42
00:05:24,580 --> 00:05:33,670
Uh, being in smooth and stable states as opposed to micro states.

43
00:05:33,670 --> 00:05:44,140
Let me zoom that back in so it's more legible. So we wrapped up last Wednesday by introducing this concept of thermodynamic equilibrium

44
00:05:44,380 --> 00:05:52,930
to just describe our physical experience and stated in the micro canonical ensemble that.

45
00:05:55,330 --> 00:06:02,320
This is defined by all of these microstate probabilities, Pi being equal,

46
00:06:03,400 --> 00:06:10,060
which is not necessarily obvious, and what we will be focusing on today, the connection between.

47
00:06:12,160 --> 00:06:17,049
This formal mathematical definition of thermodynamic equilibrium in the micro canonical ensemble,

48
00:06:17,050 --> 00:06:22,510
and how that connects to what we observe and experience in our lives.

49
00:06:22,510 --> 00:06:29,530
So any questions about all of these big new topics that have been introduced and we now get to dig into.

50
00:06:36,980 --> 00:06:48,440
So the plan for today is to continue focusing on this question of thermodynamic equilibrium and in particular approaching it through.

51
00:06:50,510 --> 00:06:57,830
A high concept or a quantity that you will have heard of before, I'm sure, called entropy.

52
00:06:59,300 --> 00:07:04,310
Um, we will get a precise mathematical definition of entropy.

53
00:07:04,610 --> 00:07:10,610
Look at its large scale properties and how the properties of entropy, um,

54
00:07:11,630 --> 00:07:17,750
resemble those of other large scale macroscopic features of statistical systems more generally,

55
00:07:18,380 --> 00:07:30,080
and possibly, um, get all the way to the second law of thermodynamics, depending on how the time goes.

56
00:07:30,080 --> 00:07:33,290
If we don't get to it today, then we certainly will on Wednesday.

57
00:07:35,730 --> 00:07:50,670
So a few more conceptual aspects of thermodynamic equilibrium that I do want to emphasise just to head off, uh, some often common misconceptions.

58
00:07:51,300 --> 00:08:02,910
So one thing to note in general terms is that the equilibrium we are talking about in statistical mechanics, it's not a matter of.

59
00:08:05,430 --> 00:08:10,490
Know, reaching a particular balance or a final ultimate state that we evolve to.

60
00:08:10,500 --> 00:08:16,980
And then we sit there and nothing happens. What we have in statistical systems is a dynamic equilibrium.

61
00:08:17,490 --> 00:08:23,370
It is not static and not a single particular microstate.

62
00:08:24,090 --> 00:08:39,900
What's going on in equilibrium is that the system does continue adopting probabilistically all of its possible microstates, Omega I.

63
00:08:42,090 --> 00:08:50,760
And it adopts those with the probability distribution corresponding to thermodynamic equilibrium situation.

64
00:08:51,270 --> 00:08:58,500
All of those probabilities being equal in the micro canonical ensemble that we'll focus on today.

65
00:08:58,860 --> 00:09:09,059
So from this dynamic equilibrium we want to explain the emergence of smooth and stable behaviour,

66
00:09:09,060 --> 00:09:12,420
like the distribution of the air molecules in this room.

67
00:09:13,140 --> 00:09:24,150
And that is where entropy is going to come in through a bit of a, um, build up.

68
00:09:24,150 --> 00:09:33,450
That will give us some practice talking about the large scale properties of thermodynamic systems.

69
00:09:34,290 --> 00:09:41,140
More generally. So.

70
00:09:43,150 --> 00:09:52,090
This is the motivation for looking at entropy as a sort of first non-trivial derived quantity of the system.

71
00:09:53,220 --> 00:09:57,820
And um, but the energy is also derived.

72
00:09:57,820 --> 00:10:05,830
But as that is conserved in the micro canonical ensemble, it has a kind of boring fixed value and nothing that interesting going on with it.

73
00:10:06,490 --> 00:10:10,240
But let's start with a formal definition of entropy.

74
00:10:12,520 --> 00:10:19,059
That will hold not just for the micro canonical ensemble, but for any statistical ensemble.

75
00:10:19,060 --> 00:10:25,480
So any collection of particles evolving in time, subject to any physical constraints.

76
00:10:26,860 --> 00:10:35,700
We will assume for simplicity in this definition that our ensemble has a countable number of microstates.

77
00:10:36,270 --> 00:10:47,650
That will be our usual capital M, and that is just for convenience, so that we can write the entropy as a sum over all of those microstates.

78
00:10:49,090 --> 00:10:55,170
So S is the symbol that is traditionally used for entropy.

79
00:10:55,180 --> 00:11:04,749
It might mean something in German. I neglected to look up what the symbol was introduced by Ludovic Boltzmann in the late 1800s.

80
00:11:04,750 --> 00:11:13,900
So summing over all of those countable microstates. And what we have in the sum is the product of that microstate probability,

81
00:11:15,100 --> 00:11:22,840
which is guaranteed to be non-zero times its logarithm, which is negative and cancels out the negative sign out front.

82
00:11:24,130 --> 00:11:35,760
So this is a non-negative quantity, and I will remind you that whenever I write log, I mean the natural log base e, I just call it log by habit.

83
00:11:35,770 --> 00:11:41,350
So if any confusion arises about that, feel free to shout out a question.

84
00:11:42,970 --> 00:11:48,580
So that is a formal definition that holds for any statistical ensemble.

85
00:11:48,970 --> 00:11:54,640
If we look at. The micro canonical ensemble.

86
00:11:58,450 --> 00:12:05,679
And you've seen that thermodynamic equilibrium in this case is defined by having

87
00:12:05,680 --> 00:12:12,790
all of these probabilities equal to one over the total number of microstates.

88
00:12:14,020 --> 00:12:20,290
If we plug that in and we get a fairly simple result.

89
00:12:20,830 --> 00:12:25,420
So we have a one over M times log, one over m.

90
00:12:25,960 --> 00:12:35,500
There's no more dependence on the microstate here. So this sum is just summing over m terms giving a factor of m that cancels the one over m.

91
00:12:36,010 --> 00:12:40,030
We are left just with negative log one over M.

92
00:12:40,900 --> 00:12:47,950
And if you haven't worked with logarithms for a while, this module will be a good opportunity to review their properties.

93
00:12:48,580 --> 00:12:54,880
A factor of the logarithm can be converted to an exponent on its argument.

94
00:12:55,240 --> 00:13:05,850
So we have the log of one over m to the minus one power, which is just log of m, and this I mentioned Boltzmann just a few minutes ago.

95
00:13:05,860 --> 00:13:09,520
This is known as Boltzmann's equation.

96
00:13:11,770 --> 00:13:14,890
And is actually printed on his grave in Vienna.

97
00:13:15,400 --> 00:13:19,120
If you are in Vienna and want to do some physics sightseeing,

98
00:13:19,810 --> 00:13:28,420
you will see the micro canonical expression for entropy on Boltzmann's grave in the cemetery there.

99
00:13:32,860 --> 00:13:37,570
Any immediate questions about this more formal definition of entropy?

100
00:13:38,950 --> 00:13:41,920
Oops. You could keep it on the screen there.

101
00:13:48,100 --> 00:13:59,620
So one other thing we can say in very general terms about entropy and any other macroscopic observable in statistical systems, um, that we.

102
00:14:01,750 --> 00:14:10,000
Are trying to explain how these can exhibit stable behaviour on the basis of randomly fluctuating underlying microstates.

103
00:14:11,530 --> 00:14:16,990
Um, that is what we mean by thermodynamic equilibrium.

104
00:14:17,710 --> 00:14:21,790
And this implies that all of these properties, like entropy.

105
00:14:24,820 --> 00:14:31,330
Will depend not on the individual microstates in great detail, but actually on.

106
00:14:33,490 --> 00:14:40,210
The quantities that we know to be stable by the construction of the statistical ensemble in general.

107
00:14:40,960 --> 00:14:47,080
So for the micro canonical ensemble, well,

108
00:14:49,000 --> 00:14:56,140
this dependence on conserved quantities for the micro canonical ensemble translates to being functions of energy and particle number.

109
00:14:58,270 --> 00:15:03,810
So so far the entropy is really the only example that we have.

110
00:15:03,820 --> 00:15:13,050
We will see more on Wednesday. So we expect to be able to express the entropy as a function of energy and particle number.

111
00:15:13,060 --> 00:15:18,160
We have expressed it as a function of the number of microstates.

112
00:15:18,820 --> 00:15:30,070
So what this is implying or telling us is maybe not too surprisingly that these properties, the conserved energy, conserved particle number,

113
00:15:30,070 --> 00:15:38,740
and total number of microstates are all interrelated, and we will be able to connect each of them to the others.

114
00:15:39,760 --> 00:15:45,820
So this is some very general statement that might benefit from a concrete example.

115
00:15:48,880 --> 00:15:56,770
So we'll go back to the spin system that was also introduced last week and see how this works out.

116
00:15:57,280 --> 00:16:04,540
We can recall that the spin system has some number of degrees of freedom that can only have two states up or down,

117
00:16:05,140 --> 00:16:18,010
plus one minus one heads or tails, and its energy is coming from an external magnetic field that, um, the spins are preferentially aligned with.

118
00:16:18,010 --> 00:16:21,700
So and spins aligned with this external field. The energy gets lowered.

119
00:16:22,030 --> 00:16:27,340
When they oppose it, the energy is higher. Let's take the very simple case.

120
00:16:28,660 --> 00:16:38,650
Suppose we just have n spins, and we take that external field and set its strength to zero, which means that the energy is always zero.

121
00:16:43,450 --> 00:16:48,130
Independent of any orientations of any of those spins.

122
00:16:48,580 --> 00:16:55,810
So what is the entropy of this situation for this particular with, uh,

123
00:16:56,510 --> 00:17:03,100
with this vanishing external field, I will ask you to make sure that everything is clear.

124
00:17:05,800 --> 00:17:09,010
And if it's not, you can feel free to tell me.

125
00:17:54,590 --> 00:18:02,650
Sera sera sera possible. It does make sense.

126
00:18:02,830 --> 00:18:07,450
But what we have is the same energy for all microstates.

127
00:18:07,880 --> 00:18:09,970
Um, so I can kind of turn that on its head.

128
00:18:10,570 --> 00:18:18,580
That it's not that the system only has the one microstate, is that all those microstates have the same zero energy and are therefore equivalent.

129
00:18:19,630 --> 00:18:31,000
So in this case, um, you know, we've seen last Wednesday that the total number of microstates that we have is two to the power N.

130
00:18:31,000 --> 00:18:40,450
So each of those end spins has two options. That is our m that goes into the entropy through the logarithm.

131
00:18:40,690 --> 00:18:52,450
So the entropy S is the logarithm of the total number of microstates that are able to conserve the energy zero, which is all of them in this case.

132
00:18:53,170 --> 00:19:01,840
Um and then properties of the log just mean that's is equal to the number of spins times the log of two.

133
00:19:03,740 --> 00:19:16,950
So let's try again. Um. So what would we have for the entropy again as a function of these conserved quantities?

134
00:19:18,030 --> 00:19:24,240
We'll keep the energy equal to zero. We'll fix the same number of spins.

135
00:19:25,080 --> 00:19:30,030
But we will now turn that external field back on.

136
00:19:30,570 --> 00:19:38,340
So h is greater than zero. And now different microstates will have different energies that depend on h.

137
00:19:39,420 --> 00:19:43,440
So e equal to zero will arise.

138
00:19:43,440 --> 00:19:49,230
Now only when there are an equal number of spins that are aligned with the magnetic field.

139
00:19:50,160 --> 00:19:56,040
So we call that. And plus imagining that a positive magnetic field is pointing in the opposite direction.

140
00:19:57,150 --> 00:20:05,370
So there is the same number of upwards pointing spins as downward pointing spins up and down between them have to account for all the spins.

141
00:20:05,730 --> 00:20:10,440
So each of those two is a half of the total capital N.

142
00:20:13,200 --> 00:20:18,000
So what is the. I'll ask you again what is the entropy in this case.

143
00:20:34,380 --> 00:20:45,360
It's a bit more complicated to figure out. So maybe if you just shout out the first step, we can.

144
00:20:47,370 --> 00:20:56,960
Step through it together. It is.

145
00:20:59,160 --> 00:21:08,310
Micro canonical. It's. Not sure why Panopto was complaining about not catching the audio, but we'll see how the recording turns out.

146
00:21:12,870 --> 00:21:16,660
So they can break down the big question into a smaller one.

147
00:21:16,710 --> 00:21:28,290
What is M? The number of accessible microstates that we have in the micro canonical ensemble that conserves

148
00:21:28,590 --> 00:21:34,560
this zero value of the energy by imposing equal numbers of upward and downward pointing spins.

149
00:21:52,500 --> 00:21:56,370
Like any of us who choose. And it is, um.

150
00:21:58,940 --> 00:22:03,880
Or. Well. Said the right words.

151
00:22:04,120 --> 00:22:07,689
And then just the notation of the choosing its end.

152
00:22:07,690 --> 00:22:11,710
Choose and over two rather and over to choose n um.

153
00:22:12,430 --> 00:22:20,320
So out of the total number of spins and we choose half of them to be either pointing up or pointing down.

154
00:22:20,980 --> 00:22:25,340
That is a binomial coefficient. We've now seen that a few times.

155
00:22:25,370 --> 00:22:40,540
This is the logarithm of say and factorial divided by n over two factorial and and minus n over two factorial is another and over two factorial.

156
00:22:42,340 --> 00:22:48,130
We can again use properties of logarithms to rearrange things just a bit.

157
00:22:49,150 --> 00:22:55,000
Um, rather than dividing by two. It might be nice to multiply by two.

158
00:22:56,350 --> 00:23:02,140
So capital n is two and plus factorial all in the log.

159
00:23:02,590 --> 00:23:07,059
And then we have two factors in the denominator.

160
00:23:07,060 --> 00:23:12,460
So with the negative sign of the log of just n plus factorial by itself.

161
00:23:14,350 --> 00:23:22,210
So once we resume doing tutorials we can work on a problem that gets this into, um,

162
00:23:23,350 --> 00:23:33,430
a more analytic form using something called Stirling's approximation for, uh, expressing factorials as nice differentiable functions.

163
00:23:34,960 --> 00:23:40,240
But for now let's just look at a particular example.

164
00:23:40,240 --> 00:23:44,440
So set. And equals eight.

165
00:23:45,190 --> 00:23:51,250
So in this case that means. And plus and minus are both just four.

166
00:23:51,970 --> 00:23:58,240
And the entropy is the logarithm of eight factorial over.

167
00:24:00,090 --> 00:24:06,000
Two powers of four factorial. So all the 432 will cancel out with one.

168
00:24:06,480 --> 00:24:09,990
So we have eight, seven, six and five.

169
00:24:10,710 --> 00:24:17,550
And then the other four factorial left in the denominator two and four will cancel eight.

170
00:24:18,330 --> 00:24:21,400
Three will take the six down to two.

171
00:24:21,420 --> 00:24:25,890
So we get 35 times two is the log of 70.

172
00:24:26,700 --> 00:24:30,239
Which means that um s is the log of m.

173
00:24:30,240 --> 00:24:35,879
In this particular case, fixing the energy to be zero with a non-zero magnetic field,

174
00:24:35,880 --> 00:24:45,840
we go down from our 256 microstates that we would have if the magnetic field were off.

175
00:24:46,410 --> 00:24:50,760
We're down to 70 here for the symmetric case.

176
00:24:51,240 --> 00:24:54,870
Equal and opposite contributions from upward and downward pointing spins.

177
00:24:55,590 --> 00:25:05,999
And if we think back to the example we worked on last Wednesday where we just picked a random configuration,

178
00:25:06,000 --> 00:25:13,740
looked at its energy that was not minus four h, and counted the number of microstates that had that fixed energy.

179
00:25:13,920 --> 00:25:19,620
So imposing conservation of energy and coming up with a total of 28 microstates,

180
00:25:20,400 --> 00:25:28,500
we can see how the total possibilities are being restricted by imposing conservation of energy.

181
00:25:29,190 --> 00:25:31,600
And we can say that, you know,

182
00:25:31,620 --> 00:25:41,040
it's more so there is more entropy from having this energy of zero than there would have been from having the 28 states that were allowed,

183
00:25:41,490 --> 00:25:46,110
just with two of the spins pointing down and an energy of minus four h.

184
00:25:48,090 --> 00:25:51,959
So there's one example.

185
00:25:51,960 --> 00:26:00,360
To make things more concrete. Let's step back now to some more, uh, generic remarks.

186
00:26:01,500 --> 00:26:14,459
So one thing that we can note about all of these calculations in the micro canonical ensemble is that as the number of microstates goes to infinity,

187
00:26:14,460 --> 00:26:19,960
whether that's countable or continuous. Then?

188
00:26:20,860 --> 00:26:28,030
Well, it's a logarithm which is equal to. The entropy is also going to diverge, albeit logarithmically slowly.

189
00:26:30,040 --> 00:26:39,400
Again this is the micro canonical ensemble imposing thermodynamic equilibrium as we will do by instinct or by default in this module.

190
00:26:41,010 --> 00:26:47,639
So this is this property of the entropy that it grows with the number of microstates.

191
00:26:47,640 --> 00:26:54,840
So the size of the system in terms of the number of possibilities is related to a

192
00:26:54,840 --> 00:27:03,750
property of the entropy and various other large scale observables known as extensively.

193
00:27:04,950 --> 00:27:12,479
And we will take a few minutes to introduce this property kind of full generality,

194
00:27:12,480 --> 00:27:19,530
that we will then be able to adapt and make use of in a few other contexts moving forward.

195
00:27:21,510 --> 00:27:24,990
So. Let's see.

196
00:27:25,110 --> 00:27:37,829
There's the screen there. The way to think about extensive or what is an extensive quantity is to set up a

197
00:27:37,830 --> 00:27:43,860
thought experiment of considering two systems that are statistically independent.

198
00:27:43,860 --> 00:27:48,810
They don't, uh, talk to each other, share information or energy in any way.

199
00:27:51,820 --> 00:28:00,400
So all of their fluctuations in thermodynamic equilibrium as they sample their own different microstates are going to be.

200
00:28:04,200 --> 00:28:07,560
Completely independent and uncorrelated.

201
00:28:08,130 --> 00:28:11,190
So here is system one.

202
00:28:13,350 --> 00:28:22,049
And for simplicity I'll say that we can imagine these are spin systems in the same magnetic field.

203
00:28:22,050 --> 00:28:25,050
So this has some number of spins.

204
00:28:25,920 --> 00:28:32,220
It has the corresponding energy that is conserved E1 and one.

205
00:28:32,700 --> 00:28:39,240
And those together give m one microstates for our system.

206
00:28:39,900 --> 00:28:49,010
Same thing from the other system, except that all the one subscripts will turn to twos,

207
00:28:49,560 --> 00:28:57,810
so e2 into spins and m to microstates for this system omega two.

208
00:28:58,980 --> 00:29:04,920
So in particular the entropy that depends on these conserved quantities.

209
00:29:05,730 --> 00:29:17,880
In system one we will have entropy S one that is just summing over all and one microstates of pi log pi.

210
00:29:19,260 --> 00:29:22,770
Same for system two though.

211
00:29:23,100 --> 00:29:26,100
Just to keep from reusing variables too much,

212
00:29:26,850 --> 00:29:41,730
I'll just label the sum with KS going from one to m2 and the probabilities will be qk just the same as pi, but with a different label to be used.

213
00:29:43,810 --> 00:29:51,460
So the trick with this setup where I forgot to put in the magnetic field there.

214
00:29:54,680 --> 00:30:01,520
The thought experiment that we can now do is to keep these systems just as they are completely independent,

215
00:30:01,520 --> 00:30:04,550
happily fluctuating in thermodynamic equilibrium on their own.

216
00:30:05,000 --> 00:30:16,940
But we can carry out our own external analysis as though they were two independent subsystems within a larger overall system.

217
00:30:18,260 --> 00:30:27,500
So we call that omega subscript one plus to say these are now subsystems.

218
00:30:30,420 --> 00:30:43,530
And try to predict the properties, such as the entropy of this combined system with two statistically independent subsystems.

219
00:30:48,830 --> 00:30:56,780
So that's our omega one plus two. And the question is what is the entropy of this system.

220
00:30:57,170 --> 00:31:08,959
S sub one plus two which we can expect will be the negative sum say now of j going from one to the total number of

221
00:31:08,960 --> 00:31:20,230
microstates and one plus two in this combined system with the probabilities times log probabilities of that system.

222
00:31:20,240 --> 00:31:24,740
And that is what we need to determine both the bounds of this summation.

223
00:31:24,950 --> 00:31:30,570
How many microstates there are and the probabilities for each of those microstates.

224
00:31:30,590 --> 00:31:44,930
What actually goes into this sum. So any questions about the basic setup before we go through and figure out how the calculation proceeds?

225
00:31:50,200 --> 00:31:55,550
Looks like we're happy with the idea. So let's implement it.

226
00:31:55,610 --> 00:32:04,280
So we'll do this. We'll get a start by just reflecting on again what statistically independent really means.

227
00:32:04,730 --> 00:32:10,520
Each system samples its microstates and thermodynamic equilibrium independently of the other.

228
00:32:10,970 --> 00:32:16,790
So whenever system one is in any given micro state.

229
00:32:16,910 --> 00:32:26,520
So. For each omega I that system one will adopt with probability pi.

230
00:32:27,240 --> 00:32:32,370
Then there are m two different microstates that system two could be in.

231
00:32:42,560 --> 00:32:50,270
So the conclusion we can draw just from thinking about this, let me know if you object,

232
00:32:51,050 --> 00:32:56,120
is that the total number of microstates in the combined system is just the product,

233
00:32:57,080 --> 00:33:03,950
the M1 microstates of omega one, each of them multiplied by the M2 microstates of omega two.

234
00:33:05,930 --> 00:33:14,900
So that's the total number of microstates we have when we think about these two subsystems as an overall combined system.

235
00:33:15,440 --> 00:33:20,390
And similarly, um, this is just the product of the total number.

236
00:33:21,170 --> 00:33:28,670
And the statistical independence tells us that the probability for each state in this product is similarly

237
00:33:28,670 --> 00:33:36,810
going to be the product of the probabilities of those two systems to be in these two microstates independently.

238
00:33:36,830 --> 00:33:47,360
So pi for omega I and system one each of the M2 microstates has that probability q okay.

239
00:33:49,010 --> 00:34:01,790
So. The probability for any one of these M1 times M2 microstates is also just pi times q okay.

240
00:34:02,810 --> 00:34:06,950
And we can do a quick sanity check, which is good practice.

241
00:34:08,630 --> 00:34:13,580
Do all of these probabilities add up to one as probabilities should?

242
00:34:15,530 --> 00:34:23,150
So we take our sum over all microstates of the probabilities p q, k.

243
00:34:24,170 --> 00:34:33,470
So that sum will go over each system independently, i.e. from one to M1K from one to m to.

244
00:34:34,400 --> 00:34:43,810
Our probabilities are p q, k. We can break this up into two sums.

245
00:34:44,530 --> 00:34:48,790
So to sum over I. Doesn't involve anything that depends on k.

246
00:34:50,380 --> 00:34:55,510
The sum over k doesn't depend on anything that involves I.

247
00:34:57,160 --> 00:35:05,650
We want to be cautious if, um, m1 and m2 are infinite, because rearranging infinite sums is a place where you can go wrong.

248
00:35:07,120 --> 00:35:11,979
It works out in this case, and if we are worried,

249
00:35:11,980 --> 00:35:19,180
we can specialise to the case where M1 and M2 are not just countable, but finite, and then take the limit at the end.

250
00:35:19,870 --> 00:35:26,079
But both of these are the probabilities of our two independent subsystems to start off with.

251
00:35:26,080 --> 00:35:30,850
So we know that they obey the requirements that probabilities sum up to one.

252
00:35:31,330 --> 00:35:34,630
So we get one times one which equals one.

253
00:35:35,530 --> 00:35:41,379
And that sanity check at least is passed for the form of the probabilities and

254
00:35:41,380 --> 00:35:46,720
microstates of our thought experiment of just saying that these two systems.

255
00:35:47,080 --> 00:35:49,300
Well, let's think about them simultaneously.

256
00:35:51,010 --> 00:36:02,050
That's really all we need to compute the entropy from the general definition that holds for any statistical ensemble.

257
00:36:02,710 --> 00:36:06,880
So similarly summing over all AI and k our.

258
00:36:08,830 --> 00:36:16,060
Probabilities are p and q k both before and after the logarithm in the sum.

259
00:36:18,010 --> 00:36:25,239
And another key property of logarithms that you will get practice with in this

260
00:36:25,240 --> 00:36:32,800
module is that the logarithm of a product is the sum of the separate logarithms.

261
00:36:33,730 --> 00:36:39,850
That's a cucu there. Hopefully that is familiar.

262
00:36:39,850 --> 00:36:46,360
You recognise it now that I mentioned it. If not, it is worth reviewing those properties of logarithms.

263
00:36:47,770 --> 00:36:57,910
We can again, as we did here, being careful about infinite sums, rearrange things to isolate the dependence on I versus k.

264
00:36:58,540 --> 00:37:06,579
So if we pull out all of the I dependent pieces from the first term,

265
00:37:06,580 --> 00:37:16,000
pick log pi the second sum over k, we'll just have a q k in there that goes to one.

266
00:37:18,640 --> 00:37:29,380
And similarly in the second term pick log UK we can pull out just pi and then everything else.

267
00:37:31,660 --> 00:37:48,450
Depends only on K. So what we have in the end is minus sum over I pi log pi, which is the entropy of that independent system.

268
00:37:48,460 --> 00:37:52,950
One other term is sum over cake log qk.

269
00:37:53,880 --> 00:37:58,350
With that negative sign is the entropy of the independent subsystem two.

270
00:37:59,760 --> 00:38:04,890
And this is what uh defines really an extensive quantity.

271
00:38:05,340 --> 00:38:10,720
So we have seen. That the entropy.

272
00:38:11,200 --> 00:38:21,730
If we consider two independent subsystems and compute the total entropy, it is just the sum of the separate entropies of those two systems.

273
00:38:22,780 --> 00:38:26,410
So this is an example of an extensive quantity.

274
00:38:26,770 --> 00:38:36,280
And the definition of that is that an extensive quantity is one that adds up across independent subsystems.

275
00:38:42,170 --> 00:38:47,930
So s of that combined system one plus two is the separate entropies,

276
00:38:48,290 --> 00:38:52,670
and the conserved quantities for the micro canonical ensemble behave the same way.

277
00:38:53,120 --> 00:38:56,929
So we could have introduced this a bit more easily.

278
00:38:56,930 --> 00:39:03,110
The energy of that combined system is the energy and the one plus the energy in the other.

279
00:39:03,680 --> 00:39:07,160
And similarly for the number of degrees of freedom,

280
00:39:08,030 --> 00:39:12,650
the total number that we get from two separate systems is just the sum of the number in each system.

281
00:39:14,330 --> 00:39:23,110
So that is. Hopefully straightforward and intuitive.

282
00:39:23,120 --> 00:39:33,560
It is not the only possibility. So somewhat in passing, there are quantities that are called intensive rather than extensive.

283
00:39:36,910 --> 00:39:46,420
And these instead of adding up the cross independent subsystems are independent of the system size or its extent.

284
00:39:52,600 --> 00:39:59,499
Which is to say that if we take these two independent subsystems, look at an intensive quantity for the combined system,

285
00:39:59,500 --> 00:40:05,890
it will be the same as for the two systems as a whole or the two systems separately.

286
00:40:06,400 --> 00:40:13,000
So examples that will hopefully make this clear are things like the temperature.

287
00:40:14,800 --> 00:40:20,580
If we take two systems at the same temperature and think about them together, they don't have double the temperature.

288
00:40:20,590 --> 00:40:23,980
The combined system has the same temperature as the independent systems.

289
00:40:24,610 --> 00:40:34,299
Similarly with things like pressure or density that we'll be working with in a few weeks, we take two systems.

290
00:40:34,300 --> 00:40:39,370
Say you take the air in this room, put a wall in between, have two halves that have the same pressure.

291
00:40:39,880 --> 00:40:48,220
We take away that wall. The pressure doesn't double. It's the same no matter how many subsystems we chop that big system into, or equivalently,

292
00:40:48,220 --> 00:40:56,080
how many independent subsystems we add up to create the overall combined system that we analyse.

293
00:40:58,340 --> 00:41:06,340
One last thing I'll say about this in general terms is that it is possible for quantities to be neither intensive nor extensive.

294
00:41:07,480 --> 00:41:14,020
And one example that we've already seen is the number of microstates, which is.

295
00:41:16,190 --> 00:41:21,020
Not the sum of the number of microstates in the two systems, but rather the product.

296
00:41:23,100 --> 00:41:32,700
So it is not the case that. All quantities will be one or the other.

297
00:41:34,920 --> 00:41:45,420
You have to analyse our observables of interest and see how they behave in order to bring this categorisation to bear.

298
00:41:46,290 --> 00:41:50,810
But it will be useful for things like entropy. Um.

299
00:41:54,070 --> 00:42:02,350
Which we will see on Wednesday looking at the time, because this is the the setup that we will use to establish the second law.

300
00:42:02,950 --> 00:42:12,330
Um. And we can lay the some of the groundwork for that in the last few minutes today.

301
00:42:15,750 --> 00:42:21,870
Let's do that by. Just checking that the.

302
00:42:25,090 --> 00:42:39,790
Results that we have derived here are sensible and, um, aligned with what we have, the understanding we've set up for somewhat dynamic equilibrium.

303
00:42:40,210 --> 00:42:45,880
So so far we've looked at these systems fairly generally as has independent quantities.

304
00:42:46,600 --> 00:42:51,580
But let's. Specialise to sort of default case.

305
00:42:53,290 --> 00:43:01,870
For this module that both omega one and omega two are independently.

306
00:43:04,790 --> 00:43:11,700
In somewhat anaemic equilibrium. So.

307
00:43:14,130 --> 00:43:20,970
This specialises us from this general case for the entropy and so on.

308
00:43:21,100 --> 00:43:34,530
It tells us that these probabilities p and q k have simple forms one over m1 and one over m2.

309
00:43:35,850 --> 00:43:42,060
The entropy is just reduced to the logs of m1 and m2.

310
00:43:44,920 --> 00:44:00,430
And if we consider this the combined probability peak when we think about the overall system as a single object with two independent subsystems.

311
00:44:02,590 --> 00:44:11,860
What we get is well, one over M1 times one over M2, which is just one over M1 plus two.

312
00:44:11,890 --> 00:44:22,070
So. From us, from supposing that the independent subsystems were separately in somewhat dynamic equilibrium,

313
00:44:22,070 --> 00:44:29,240
we have obtained an expression that says that the overall combine system is also thought of an equilibrium.

314
00:44:29,900 --> 00:44:32,210
Not too surprising, but a good sanity check.

315
00:44:32,480 --> 00:44:42,290
Again, um, all of the microstate probabilities for the M1 times M2 microstates have the same equal probability.

316
00:44:44,030 --> 00:44:50,270
So the overall system is also in thermodynamic equilibrium, as it should be.

317
00:44:50,630 --> 00:45:00,830
This is unaffected by, you know, how we choose to look at the overall system that we have to deal with, and we can check that.

318
00:45:03,400 --> 00:45:08,240
The expression for the entropy as just the log of m.

319
00:45:08,260 --> 00:45:20,740
If we plug in m1 times m2. Into that log and use the properties of the one of logarithms, we have m log one,

320
00:45:21,220 --> 00:45:30,100
sorry log m one plus log m two is just the same result that we found earlier for the sensitivity of the entropy.

321
00:45:31,370 --> 00:45:37,520
Now imposing a somewhat anaemic equilibrium and simplifying the calculation.

322
00:45:41,050 --> 00:45:49,310
So the next step to. Um, to deal with this general framework.

323
00:45:49,940 --> 00:45:55,850
We have taken our two subsystems. We've supposed they are in of dynamic equilibrium.

324
00:45:56,390 --> 00:46:07,690
What we will do on Wednesday. We will take those two subsystems and actually allow them to exchange energy with each other.

325
00:46:10,270 --> 00:46:14,200
So we'll say those two subsystems will be put in thermal contact.

326
00:46:16,900 --> 00:46:22,180
That allows them to. Exchange energy while.

327
00:46:24,280 --> 00:46:28,870
Keeping all of the the particle numbers the same and separate.

328
00:46:31,420 --> 00:46:39,610
So you know getting back to our picture of the micro canonical ensemble or just these two boxes,

329
00:46:40,540 --> 00:46:43,350
instead of having those boxes will separated and insulated,

330
00:46:43,360 --> 00:46:50,320
we will bring them together, have them share a wall of allow energy to go back and forth through that wall,

331
00:46:50,770 --> 00:46:58,720
while still leaving the total number of spins or other particles fixed.

332
00:46:59,530 --> 00:47:05,979
And we will, um, look through that set up to see how the energy behaves.

333
00:47:05,980 --> 00:47:14,350
In that case, um, it is possible that, you know, once these systems are brought into thermal contact,

334
00:47:14,350 --> 00:47:20,980
that combined system now that is able to exchange energy will no longer remain in somewhat dynamic equilibrium.

335
00:47:22,150 --> 00:47:35,520
So we will have to. Wait for that combined system to re equilibrate to bring our, uh, statistical ensemble toolbox into force.

336
00:47:35,880 --> 00:47:43,590
And we are unfortunately out of time to do that analysis today, but it will be the very first thing on Wednesday.

337
00:47:44,280 --> 00:47:53,040
So just to wrap up for today, having introduced entropy and look at his properties as an extensive quantity,

338
00:47:53,460 --> 00:47:57,090
um, are there any questions about what we've seen so far?

339
00:48:02,420 --> 00:48:07,280
Not seeing any. I'll let you go and see you again on Wednesday morning.

